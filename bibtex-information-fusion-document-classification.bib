@Article{wangq2022,
  author    = {Qian Wang and Toby P. Breckon},
  journal   = {Pattern Recognition},
  title     = {Cross-domain structure preserving projection for heterogeneous domain adaptation},
  year      = {2022},
  issn      = {0031-3203},
  month     = mar,
  pages     = {108362},
  volume    = {123},
  abstract  = {Heterogeneous Domain Adaptation (HDA) addresses the transfer learning problems where data from the source and target domains are of different modalities (e.g., texts and images) or feature dimensions (e.g., features extracted with different methods). It is useful for multi-modal data analysis. Traditional domain adaptation algorithms assume that the representations of source and target samples reside in the same feature space, hence are likely to fail in solving the heterogeneous domain adaptation problem. Contemporary state-of-the-art HDA approaches are usually composed of complex optimization objectives for favourable performance and are therefore computationally expensive and less generalizable. To address these issues, we propose a novel Cross-Domain Structure Preserving Projection (CDSPP) algorithm for HDA. As an extension of the classic LPP to heterogeneous domains, CDSPP aims to learn domain-specific projections to map sample features from source and target domains into a common subspace such that the class consistency is preserved and data distributions are sufficiently aligned. CDSPP is simple and has deterministic solutions by solving a generalized eigenvalue problem. It is naturally suitable for supervised HDA but has also been extended for semi-supervised HDA where the unlabelled target domain samples are available. Extensive experiments have been conducted on commonly used benchmark datasets (i.e. Office-Caltech, Multilingual Reuters Collection, NUS-WIDE-ImageNet) for HDA as well as the Office-Home dataset firstly introduced for HDA by ourselves due to its significantly larger number of classes than the existing ones (65 vs 10, 6 and 8). The experimental results of both supervised and semi-supervised HDA demonstrate the superior performance of our proposed method against contemporary state-of-the-art methods.},
  doi       = {10.1016/j.patcog.2021.108362},
  keywords  = {Cross-domain projection,Heterogeneous domain adaptation,Image classification,Text classification},
  publisher = {Elsevier BV},
  url       = {https://linkinghub.elsevier.com/retrieve/pii/S0031320321005422},
}

@Article{zhao2023,
  author    = {Feng Zhao and Jiahui Zhang and Zhiyuan Chen and Xiaofeng Zhang and Qingsong Xie},
  journal   = {Expert Systems},
  title     = {Topic identification of text‐based expert stock comments using multi‐level information fusion},
  year      = {2020},
  issn      = {1468-0394},
  month     = oct,
  number    = {2},
  volume    = {40},
  abstract  = {<p> Stock investment is an important mode of asset allocation and a crucial means of financial management. How to grasp the movement of stock price and predict its trend have been the focus of investors and investment companies. Since expert stock comments contain abundant essential information for investment decisions, how to identify the topic of expert stock comments with high precision and efficiency is an important research topic. However, the existing methods usually employ single feature selection strategies for topic identification of stock comments, which may lead to low accuracy. Thus, to deal with this limitation, we propose a multi‐level information fusion method to construct a topic identification system of stock comments. Specifically, we <italic>firstly</italic> fuse various complementary feature selection methods via a multi‐view learning framework which can comprehensively represent text‐based topics. <italic>In addition</italic> , regarding the decision process, we propose a fusion strategy based on belief value which can further improve the classification performance. The experimental results indicate that the proposed multi‐level information fusion method is not only superior to other methods in terms of classification, it is also able to accurately capture topics of expert stock comments. </p>},
  doi       = {10.1111/exsy.12641},
  issue     = {2},
  keywords  = {belief value,multi-level information fusion,network stock comments,text classification},
  publisher = {Wiley},
}

@Article{cgoncalves2022,
  author    = {Carlos Adriano Gonçalves and Adrián Seara Vieira and Célia Talma Gonçalves and Rui Camacho and Eva Lorenzo Iglesias and Lourdes Borrajo Diz},
  journal   = {Information},
  title     = {A Novel Multi-View Ensemble Learning Architecture to Improve the Structured Text Classification},
  year      = {2022},
  issn      = {2078-2489},
  month     = jun,
  number    = {6},
  pages     = {283},
  volume    = {13},
  abstract  = {<p>Multi-view ensemble learning exploits the information of data views. To test its efficiency for full text classification, a technique has been implemented where the views correspond to the document sections. For classification and prediction, we use a stacking generalization based on the idea that different learning algorithms provide complementary explanations of the data. The present study implements the stacking approach using support vector machine algorithms as the baseline and a C4.5 implementation as the meta-learner. Views are created with OHSUMED biomedical full text documents. Experimental results lead to the sustained conclusion that the application of multi-view techniques to full texts significantly improves the task of text classification, providing a significant contribution for the biomedical text mining research. We also have evidence to conclude that enriched datasets with text from certain sections are better than using only titles and abstracts.</p>},
  doi       = {10.3390/info13060283},
  issue     = {6},
  keywords  = {OHSUMED corpus,ensemble methods,full text classification,multi-view ensemble learning,stacking},
  publisher = {MDPI AG},
  url       = {https://www.mdpi.com/2078-2489/13/6/283},
}

@Article{reil2023,
  author    = {Luis Rei and Dunja Mladenic and Mareike Dorozynski and Franz Rottensteiner and Thomas Schleider and Raphaël Troncy and Jorge Sebastián Lozano and Mar Gaitán Salvatella},
  journal   = {Multimedia Systems},
  title     = {Multimodal metadata assignment for cultural heritage artifacts},
  year      = {2022},
  issn      = {1432-1882},
  month     = nov,
  number    = {2},
  pages     = {847--869},
  volume    = {29},
  abstract  = {We develop a multimodal classifier for the cultural heritage domain using a late fusion approach and introduce a novel dataset. The three modalities are Image, Text, and Tabular data. We based the image classifier on a ResNet convolutional neural network architecture and the text classifier on a multilingual transformer architecture (XML-Roberta). Both are trained as multitask classifiers. Tabular data and late fusion are handled by Gradient Tree Boosting. We also show how we leveraged a specific data model and taxonomy in a Knowledge Graph to create the dataset and to store classification results.},
  doi       = {10.1007/s00530-022-01025-2},
  issue     = {2},
  keywords  = {Convolutional neural networks,Cultural heritage,Deep learning,Image classification,Multilingual,Multimodal,Text classification,Transformer},
  publisher = {Springer Science and Business Media LLC},
}

@Article{debreuij2020,
  author    = {Jens A. de Bruijn and Hans de Moel and Albrecht H. Weerts and Marleen C. de Ruiter and Erkan Basar and Dirk Eilander and Jeroen C. J. H. Aerts},
  journal   = {Computers &amp; Geosciences},
  title     = {Improving the classification of flood tweets with contextual hydrological information in a multimodal neural network},
  year      = {2020},
  issn      = {0098-3004},
  month     = {7},
  pages     = {104485},
  volume    = {140},
  abstract  = {While text classification can classify tweets, assessing whether a tweet is related to an ongoing flood event or not, based on its text, remains difficult. Inclusion of contextual hydrological information could improve the performance of such algorithms. Here, a multilingual multimodal neural network is designed that can effectively use both textual and hydrological information. The classification data was obtained from Twitter using flood-related keywords in English, French, Spanish and Indonesian. Subsequently, hydrological information was extracted from a global precipitation dataset based on the tweet's timestamp and locations mentioned in its text. Three experiments were performed analyzing precision, recall and F1-scores while comparing a neural network that uses hydrological information against a neural network that does not. Results showed that F1-scores improved significantly across all experiments. Most notably, when optimizing for precision the neural network with hydrological information could achieve a precision of 0.91 while the neural network without hydrological information failed to effectively optimize. Moreover, this study shows that including hydrological information can assist in the translation of the classification algorithm to unseen languages.},
  doi       = {10.1016/j.cageo.2020.104485},
  keywords  = {Authoritative data,Early warning,Floods,Multimodal fusion,Natural hazards,Neural networks,Text classification,Word embeddings},
  publisher = {Elsevier BV},
}

@Article{liuw2021,
  author    = {Wenfu Liu and Jianmin Pang and Nan Li and Xin Zhou and Feng Yue},
  journal   = {International Journal of Computational Intelligence Systems},
  title     = {Research on Multi-label Text Classification Method Based on tALBERT-CNN},
  year      = {2021},
  issn      = {1875-6883},
  month     = dec,
  number    = {1},
  pages     = {201},
  volume    = {14},
  abstract  = {<p>Single-label classification technology has difficulty meeting the needs of text classification, and multi-label text classification has become an important research issue in natural language processing (NLP). Extracting semantic features from different levels and granularities of text is a basic and key task in multi-label text classification research. A topic model is an effective method for the automatic organization and induction of text information. It can reveal the latent semantics of documents and analyze the topics contained in massive information. Therefore, this paper proposes a multi-label text classification method based on tALBERT-CNN: an LDA topic model and ALBERT model are used to obtain the topic vector and semantic context vector of each word (document), a certain fusion mechanism is adopted to obtain in-depth topic and semantic representations of the document, and the multi-label features of the text are extracted through the TextCNN model to train a multi-label classifier. The experimental results obtained on standard datasets show that the proposed method can extract multi-label features from documents, and its performance is better than that of the existing state-of-the-art multi-label text classification algorithms.</p>},
  doi       = {10.1007/s44196-021-00055-4},
  issue     = {1},
  keywords  = {ALBERT,Fusion mechanism,Multi-label,Text classification,Topic model},
  publisher = {Springer Science and Business Media LLC},
}

@Article{guod2023,
  author    = {Dongyue Guo and Jianwei Zhang and Bo Yang and Yi Lin},
  journal   = {ACM Transactions on Asian and Low-Resource Language Information Processing},
  title     = {A Comparative Study of Speaker Role Identification in Air Traffic Communication Using Deep Learning Approaches},
  year      = {2023},
  issn      = {2375-4702},
  month     = mar,
  number    = {4},
  pages     = {1--17},
  volume    = {22},
  abstract  = {<p>Automatic spoken instruction understanding (SIU) of the controller-pilot conversations in the air traffic control (ATC) requires not only recognizing the words and semantics of the speech but also determining the role of the speaker. However, few of the published works on the automatic understanding systems in air traffic communication focus on speaker role identification (SRI). In this article, we formulate the SRI task of controller-pilot communication as a binary classification problem. Furthermore, the text-based, speech-based, and speech-and-text-based multi-modal methods are proposed to achieve a comprehensive comparison of the SRI task. To ablate the impacts of the comparative approaches, various advanced neural network architectures are applied to optimize the implementation of text-based and speech-based methods. Most importantly, a multi-modal speaker role identification network (MMSRINet) is designed to achieve the SRI task by considering both the speech and textual modality features. To aggregate modality features, the modal fusion module is proposed to fuse and squeeze acoustic and textual representations by modal attention mechanism and self-attention pooling layer, respectively. Finally, the comparative approaches are validated on the ATCSpeech corpus collected from a real-world ATC environment. The experimental results demonstrate that all the comparative approaches worked for the SRI task, and the proposed MMSRINet shows competitive performance and robustness compared with the other methods on both seen and unseen data, achieving 98.56% and 98.08% accuracy, respectively.</p>},
  doi       = {10.1145/3572792},
  issue     = {4},
  keywords  = {Speaker role identification,air traffic control,multi-modal learning,speech classification,spoken instruction understanding,text classification},
  publisher = {Association for Computing Machinery (ACM)},
}

@InProceedings{chatziagapia2022,
  author    = {Aggelina Chatziagapi and Dimitris Sgouropoulos and Constantinos Karouzos and Thomas Melistas and Theodoros Giannakopoulos and Athanasios Katsamanis and Shrikanth Narayanan},
  booktitle = {2022 10th International Conference on Affective Computing and Intelligent Interaction (ACII)},
  title     = {Audio and ASR-based Filled Pause Detection},
  year      = {2022},
  month     = oct,
  pages     = {1--7},
  publisher = {IEEE},
  abstract  = {Filled pauses (or fillers) are the most common form of speech disfluencies and they can be recognized as hesitation markers ('um', 'uh' and 'er') made by speakers, usually to gain extra time while thinking their next words. Filled pauses are very frequent in spontaneous speech. Their detection is therefore rather important for two basic reasons: (a) their existence influences the performance of individual components, like Automatic Speech Recognition system (ASR), in human-machine interaction and (b) their frequency can characterize the overall speech quality of a particular speaker, as it can be strongly associated with the speaker's confidence. Despite that, only limited work has been published for the detection of filled pauses in speech, especially through audio. In this work, we propose a framework for filled pause detection using both audio and textual information. For the audio modality, we transfer knowledge from a plethora of supervised tasks, such as emotion or speaking rate, using Convolutional Neural Networks (CNNs). For the text modality, we develop a temporal Recurrent Neural Network (RNN) method that takes into account textual information derived from an ASR system. In addition, the proposed transfer learning approach for the audio classifier leads to better results when benchmarked on our internal dataset for which the text is not transcribed but estimated by an ASR system. In this case, a simple late fusion approach boosts the performance even further. This proves that the audio approach is suitable for real-world applications where the transcribed text is not available and has to leverage imperfect ASR results, or even the absence of textual information (to reduce computational cost).},
  doi       = {10.1109/acii55700.2022.9953889},
  isbn      = {9781665459082},
  journal   = {2022 10th International Conference on Affective Computing and Intelligent Interaction, ACII 2022},
  keywords  = {Audio Classification,Automatic Speech Recognition,Convolutional Neural Networks,Deep Learning,Disfluency Detection,Filled Pauses,Hesitation,Multimodal Learning,Recurrent Neural Networks,Text Classification},
}

@Article{andriyanovn2022,
  author    = {N. A. Andriyanov},
  journal   = {Pattern Recognition and Image Analysis},
  title     = {Combining Text and Image Analysis Methods for Solving Multimodal Classification Problems},
  year      = {2022},
  issn      = {1555-6212},
  month     = sep,
  number    = {3},
  pages     = {489--494},
  volume    = {32},
  abstract  = {Abstract: Fairly large number of recent studies are devoted to the analysis of data containing heterogeneous information. Multimodality is considered by scientists as a step towards artificial general intelligence. In this article, we study the problem of classifying images containing text inserts. At the same time, the results for joint classification by text and image outperform the image classification algorithm by about 5%, and the text classification algorithm by 8%. Moreover, the share of correct recognitions for the proposed model in the problem of partitioning into 3 classes is 86%.},
  doi       = {10.1134/s1054661822030026},
  issue     = {3},
  keywords  = {aggregation of classifier predictions,image recognition,lemmatization,multimodal data processing,natural language processing,optical character recognition,text vectorization,tokenization},
  publisher = {Pleiades Publishing Ltd},
}

@Article{jiangs2024,
  author    = {Shuo Jiang and Jie Hu and Christopher L. Magee and Jianxi Luo},
  journal   = {IEEE Transactions on Engineering Management},
  title     = {Deep Learning for Technical Document Classification},
  year      = {2024},
  issn      = {1558-0040},
  pages     = {1163--1179},
  volume    = {71},
  abstract  = {In large technology companies, the requirements for managing and organizing technical documents created by engineers and managers have increased dramatically in recent years, which has led to a higher demand for more scalable, accurate, and automated document classification. Prior studies have only focused on processing text for classification, whereas technical documents often contain multimodal information. To leverage multimodal information for document classification to improve the model performance, this article presents a novel multimodal deep learning architecture, i.e., TechDoc, which utilizes three types of information, including natural language texts and descriptive images within documents and the associations among the documents. The architecture synthesizes the convolutional neural network, recurrent neural network, and graph neural network through an integrated training process. We applied the architecture to a large multimodal technical document database and trained the model for classifying documents based on the hierarchical International Patent Classification system. Our results show that TechDoc presents a greater classification accuracy than the unimodal methods and other state-of-the-art benchmarks. The trained model can potentially be scaled to millions of real-world multimodal technical documents, which is useful for data and knowledge management in large technology companies and organizations.},
  doi       = {10.1109/tem.2022.3152216},
  keywords  = {Artificial intelligence,deep learning,document classification,neural networks,technology management},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{luox2022,
  author    = {Xiong Luo and Zhijian Yu and Zhigang Zhao and Wenbing Zhao and Jenq-Haur Wang},
  journal   = {Digital Communications and Networks},
  title     = {Effective short text classification via the fusion of hybrid features for IoT social data},
  year      = {2022},
  issn      = {2352-8648},
  month     = dec,
  number    = {6},
  pages     = {942--954},
  volume    = {8},
  abstract  = {Nowadays short texts can be widely found in various social data in relation to the 5G-enabled Internet of Things (IoT). Short text classification is a challenging task due to its sparsity and the lack of context. Previous studies mainly tackle these problems by enhancing the semantic information or the statistical information individually. However, the improvement achieved by a single type of information is limited, while fusing various information may help to improve the classification accuracy more effectively. To fuse various information for short text classification, this article proposes a feature fusion method that integrates the statistical feature and the comprehensive semantic feature together by using the weighting mechanism and deep learning models. In the proposed method, we apply Bidirectional Encoder Representations from Transformers (BERT) to generate word vectors on the sentence level automatically, and then obtain the statistical feature, the local semantic feature and the overall semantic feature using Term Frequency-Inverse Document Frequency (TF-IDF) weighting approach, Convolutional Neural Network (CNN) and Bidirectional Gate Recurrent Unit (BiGRU). Then, the fusion feature is accordingly obtained for classification. Experiments are conducted on five popular short text classification datasets and a 5G-enabled IoT social dataset and the results show that our proposed method effectively improves the classification performance.},
  doi       = {10.1016/j.dcan.2022.09.015},
  issue     = {6},
  keywords  = {BERT,Bidirectional encoder representations from transformers,Deep learning,Information fusion,Short text classification,Social data},
  publisher = {Elsevier BV},
}

@Article{kanchid2022,
  author    = {Shrinidhi Kanchi and Alain Pagani and Hamam Mokayed and Marcus Liwicki and Didier Stricker and Muhammad Zeshan Afzal},
  journal   = {Applied Sciences},
  title     = {EmmDocClassifier: Efficient Multimodal Document Image Classifier for Scarce Data},
  year      = {2022},
  issn      = {2076-3417},
  month     = jan,
  number    = {3},
  pages     = {1457},
  volume    = {12},
  abstract  = {Document classification is one of the most critical steps in the document analysis pipeline. There are two types of approaches for document classification, known as image-based and multi-modal approaches. Image-based document classification approaches are solely based on the inherent visual cues of the document images. In contrast, the multimodal approach co-learns the visual and textual features, and it has proved to be more effective. Nonetheless, these approaches require a huge amount of data. This paper presents a novel approach for document classification that works with a small amount of data and outperforms other approaches. The proposed approach incorporates a hierarchical attention network (HAN) for the textual stream and the EfficientNet-B0 for the image stream. The hierarchical attention network in the textual stream uses dynamic word embedding through fine-tuned BERT. HAN incorporates both the word level and sentence level features. While earlier approaches rely on training on a large corpus (RVL-CDIP), we show that our approach works with a small amount of data (Tobacco-3482). To this end, we trained the neural network at Tobacco-3482 from scratch. Therefore, we outperform the state-of-the-art by obtaining an accuracy of 90.3%. This results in a relative error reduction rate of 7.9%.},
  doi       = {10.3390/app12031457},
  issue     = {3},
  keywords  = {BERT,Document image classification,EfficientNet,Fine-tuned BERT,Hierarchical attention networks,Multimodal,RVL-CDIP,Tobacco-3482,Two-stream},
  publisher = {MDPI AG},
}

@Article{adwaithd2022,
  author    = {Divakaran Adwaith and Ashok Kumar Abishake and Siva Venkatesh Raghul and Elango Sivasankar},
  journal   = {Multimedia Tools and Applications},
  title     = {Enhancing multimodal disaster tweet classification using state-of-the-art deep learning networks},
  year      = {2022},
  issn      = {1573-7721},
  month     = mar,
  number    = {13},
  pages     = {18483--18501},
  volume    = {81},
  abstract  = {During disasters, multimedia content on social media sites offers vital information. Reports of injured or deceased people, infrastructure destruction, and missing or found people are among the types of information exchanged. While several studies have demonstrated the importance of both text and picture content for disaster response, previous research has primarily concentrated on the text modality and not so much success with multi-modality. Latest research in multi-modal classification in disaster related tweets uses comparatively primitive models such as KIMCNN and VGG16. In this research work we have taken this further and utilized state-of-the-art models in both text and image classification to try and improve multi-modal classification of disaster related tweets. The research was conducted on two different classification tasks, first to detect if a tweet is informative or not, second to understand the response needed. The process of multimodal analysis is broken down by incorporating different methods of feature extraction from the textual data corpus and pre-processing the corresponding image corpus, then we use several classification models to train and predict the output and compare their performances while tweaking the parameters to improve the results. Models such as XLNet, BERT and RoBERTa in text classification and ResNet, ResNeXt and DenseNet in image classification were trained and analyzed. Results show that the proposed multimodal architecture outperforms models trained using a single modality (text or image alone). Also, it proves that the newer state-of-the-art models outperform the baseline models by a reasonable margin for both the classification tasks.},
  doi       = {10.1007/s11042-022-12217-3},
  issue     = {13},
  keywords  = {Deep learning,Disaster response,Multimodal analysis,Tweet classification},
  publisher = {Springer Science and Business Media LLC},
}

@Article{liuj2022,
  author    = {Jing Liu and Yue Wang and Lihua Huang and Chenghong Zhang and Songzheng Zhao},
  journal   = {Information},
  title     = {Identifying Adverse Drug Reaction-Related Text from Social Media: A Multi-View Active Learning Approach with Various Document Representations},
  year      = {2022},
  issn      = {2078-2489},
  month     = apr,
  number    = {4},
  pages     = {189},
  volume    = {13},
  abstract  = {Adverse drug reactions (ADRs) are a huge public health issue. Identifying text that mentions ADRs from a large volume of social media data is important. However, we need to address two challenges for high-performing ADR-related text detection: the data imbalance problem and the requirement of simultaneously using data-driven information and handcrafted information. Therefore, we propose an approach named multi-view active learning using domain-specific and data-driven document representations (MVAL4D), endeavoring to enhance the predictive capability and alleviate the requirement of labeled data. Specifically, a new view-generation mechanism is proposed to generate multiple views by simultaneously exploiting various document representations obtained using handcrafted feature engineering and by performing deep learning methods. Moreover, different from previous active learning studies in which all instances are chosen using the same selection criterion, MVAL4D adopts different criteria (i.e., confidence and informative-ness) to select potentially positive instances and potentially negative instances for manual annotation. The experimental results verify the effectiveness of MVAL4D. The proposed approach can be generalized to many other text classification tasks. Moreover, it can offer a solid foundation for the ADR mention extraction task, and improve the feasibility of monitoring drug safety using social media data.},
  doi       = {10.3390/info13040189},
  issue     = {4},
  keywords  = {adverse drug reaction,document representation,multi-view active learning,selection strategy},
  publisher = {MDPI AG},
}

@InProceedings{sangy2022,
  author    = {Yisi Sang and Xiangyang Mou and Mo Yu and Dakuo Wang and Jing Li and Jeffrey Stanton},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
  title     = {MBTI Personality Prediction for Fictional Characters Using Movie Scripts},
  year      = {2022},
  pages     = {6715--6724},
  publisher = {Association for Computational Linguistics (ACL)},
  abstract  = {An NLP model that understands stories should be able to understand the characters in them. To support the development of neural models for this purpose, we construct a benchmark, Story2Personality. The task is to predict a movie character's MBTI or Big 5 personality types based on the narratives of the character. Experiments show that our task is challenging for the existing text classification models, as none is able to largely outperform random guesses. We further proposed a multi-view model for personality prediction using both verbal and non-verbal descriptions, which gives improvement compared to using only verbal descriptions. The uniqueness and challenges in our dataset call for the development of narrative comprehension techniques from the perspective of understanding characters.},
  doi       = {10.18653/v1/2022.findings-emnlp.500},
  journal   = {Findings of the Association for Computational Linguistics: EMNLP 2022},
}

@Article{paraskevopoulos2022,
  author    = {Georgios Paraskevopoulos and Petros Pistofidis and Georgios Banoutsos and Efthymios Georgiou and Vassilis Katsouros},
  journal   = {Applied Sciences},
  title     = {Multimodal Classification of Safety-Report Observations},
  year      = {2022},
  issn      = {2076-3417},
  month     = jun,
  number    = {12},
  pages     = {5781},
  volume    = {12},
  abstract  = {Modern businesses are obligated to conform to regulations to prevent physical injuries and ill health for anyone present on a site under their responsibility, such as customers, employees and visitors. Safety officers (SOs) are engineers, who perform site audits to businesses, record observations regarding possible safety issues and make appropriate recommendations. In this work, we develop a multimodal machine-learning architecture for the analysis and categorization of safety observations, given textual descriptions and images taken from the location sites. For this, we utilize a new multimodal dataset, Safety4All, which contains 5344 safety-related observations created by 86 SOs in 486 sites. An observation consists of a short issue description, written by the SOs, accompanied with images where the issue is shown, relevant metadata and a priority score. Our proposed architecture is based on the joint fine tuning of large pretrained language and image neural network models. Specifically, we propose the use of a joint task and contrastive loss, which aligns the text and vision representations in a joint multimodal space. The contrastive loss ensures that inter-modality representation distances are maintained, so that vision and language representations for similar samples are close in the shared multimodal space. We evaluate the proposed model on three tasks, namely, priority classification of input observations, observation assessment and observation categorization. Our experiments show that inspection scene images and textual descriptions provide complementary information, signifying the importance of both modalities. Furthermore, the use of the joint contrastive loss produces strong multimodal representations and outperforms a baseline simple model in tasks fusion. In addition, we train and release a large transformer-based language model for the Greek language based on the Electra architecture.},
  doi       = {10.3390/app12125781},
  issue     = {12},
  keywords  = {contrastive learning,multimodal fusion,occupational safety and health (OSH),safety reports,text classification,text–visual},
  publisher = {MDPI AG},
}

@Article{sapeao2022,
  author    = {Oscar Sapena and Eva Onaindia},
  journal   = {Applied Sciences},
  title     = {Multimodal Classification of Teaching Activities from University Lecture Recordings},
  year      = {2022},
  issn      = {2076-3417},
  month     = may,
  number    = {9},
  pages     = {4785},
  volume    = {12},
  abstract  = {The way of understanding online higher education has greatly changed due to the world-wide pandemic situation. Teaching is undertaken remotely, and the faculty incorporate lecture audio recordings as part of the teaching material. This new online teaching–learning setting has largely impacted university classes. While online teaching technology that enriches virtual classrooms has been abundant over the past two years, the same has not occurred in supporting students during online learning. To overcome this limitation, our aim is to work toward enabling students to easily access the piece of the lesson recording in which the teacher explains a theoretical concept, solves an exercise, or comments on organizational issues of the course. To that end, we present a multimodal classification algorithm that identifies the type of activity that is being carried out at any time of the lesson by using a transformer-based language model that exploits features from the audio file and from the automated lecture transcription. The experimental results will show that some academic activities are more easily identifiable with the audio signal while resorting to the text transcription is needed to identify others. All in all, our contribution aims to recognize the academic activities of a teacher during a lesson.},
  doi       = {10.3390/app12094785},
  issue     = {9},
  keywords  = {audio processing,class recordings,intelligent online learning,natural language processing,text classification,transformer models},
  publisher = {MDPI AG},
}

@InBook{arlqaraleshs2024,
  author    = {Saed Alqaraleh and Hatice Sirin},
  pages     = {1--13},
  publisher = {Springer Nature Switzerland},
  title     = {Multimodal Classifier for Disaster Response},
  year      = {2023},
  isbn      = {9783031509209},
  month     = dec,
  volume    = {1983 CCIS},
  abstract  = {Data obtained from social media has a massive effect on making correct decisions in time-critical situations and natural disasters. Social media content generally consists of messages, images, and videos. In situations of disasters, using multimedia files such as images can significantly help in understanding the damage caused by disasters compared to using text only. In other words, the exact situation and the effect of disaster are better understood using visual data. So far, researchers widely use text datasets for building efficient disaster management systems, and a limited number of studies have focused on using other content, such as images and videos. This is due to the lack of available multimodal datasets. We addressed this limitation in this work by introducing a new Turkish multimodal dataset. This dataset was created by collecting disaster-related Turkish texts and their related images from Twitter. Then, by three evaluators and the majority voting, each sample was annotated as a disaster or not a disaster. Next, multimodal classification studies were carried out with the late fusion technique. The BERT embedding approach and a pre-trained LSTM model are used to classify the text, and a pre-trained CNN model is used for the visual content (images). Overall, concatenating both inputs in a multimodal learning architecture using late fusion achieved an accuracy of 91.87% compared to early fusion, which achieved 86.72%.},
  booktitle = {Advanced Engineering, Technology and Applications},
  doi       = {10.1007/978-3-031-50920-9_1},
  issn      = {1865-0937},
  journal   = {Communications in Computer and Information Science},
  keywords  = {Disaster Management,Image Classification,Multimodal Classifier,Turkish language,Tweet Text Classification},
}

@InProceedings{karisanip2022,
  author    = {Payam Karisani and Negin Karisani and Li Xiong},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
  title     = {Multi-View Active Learning for Short Text Classification in User-Generated Data},
  year      = {2022},
  pages     = {6441--6453},
  publisher = {Association for Computational Linguistics (ACL)},
  abstract  = {Mining user-generated data often suffers from the lack of enough labeled data, short document lengths, and the informal user language. In this paper, we propose a novel active learning model to overcome these obstacles in the tasks tailored for query phrases-e.g., detecting positive reports of natural disasters. Our model has three novelties: 1) It is the first approach to employ multi-view active learning in this domain. 2) It uses the Parzen-Rosenblatt window method to integrate the representativeness measure into multi-view active learning. 3) It employs a query-by-committee strategy, based on the agreement between predictors, to address the usually noisy language of the documents in this domain. We evaluate our model in four publicly available Twitter datasets with distinctly different applications. We also compare our model with a wide range of baselines including those with multiple classifiers. The experiments testify that our model is highly consistent and outperforms existing models.},
  doi       = {10.18653/v1/2022.findings-emnlp.481},
  journal   = {Findings of the Association for Computational Linguistics: EMNLP 2022},
}

@Article{yuet2022,
  author    = {Tan Yue and Yong Li and Xuzhao Shi and Jiedong Qin and Zijiao Fan and Zonghai Hu},
  journal   = {Applied Sciences},
  title     = {PaperNet: A Dataset and Benchmark for Fine-Grained Paper Classification},
  year      = {2022},
  issn      = {2076-3417},
  month     = apr,
  number    = {9},
  pages     = {4554},
  volume    = {12},
  abstract  = {Document classification is an important area in Natural Language Processing (NLP). Because a huge amount of scientific papers have been published at an accelerating rate, it is beneficial to carry out intelligent paper classifications, especially fine-grained classification for researchers. However, a public scientific paper dataset for fine-grained classification is still lacking, so the existing document classification methods have not been put to the test. To fill this vacancy, we designed and collected the PaperNet-Dataset that consists of multi-modal data (texts and figures). PaperNet 1.0 version contains hierarchical categories of papers in the fields of computer vision (CV) and NLP, 2 coarse-grained and 20 fine-grained (7 in CV and 13 in NLP). We ran current mainstream models on the PaperNet-Dataset, along with a multi-modal method that we propose. Interestingly, none of these methods reaches an accuracy of 80% in fine-grained classification, showing plenty of room for improvement. We hope that PaperNet-Dataset will inspire more work in this challenging area.},
  doi       = {10.3390/app12094554},
  issue     = {9},
  keywords  = {artificial intelligence application,dataset,machine learning,multi-modal information processing,paper classification},
  publisher = {MDPI AG},
}

@InProceedings{guq2022,
  author    = {Gu, Qin and Meisinger, Nino and Dick, Anna-Katharina},
  booktitle = {Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)},
  title     = {QiNiAn at SemEval-2022 Task 5: Multi-Modal Misogyny Detection and Classification},
  year      = {2022},
  pages     = {736--741},
  publisher = {Association for Computational Linguistics (ACL)},
  abstract  = {In this paper, we describe our submission to the misogyny classification challenge at SemEval-2022. We propose two models for the two subtasks of the challenge: The first uses joint image and text classification to classify memes as either misogynistic or not. This model uses a majority voting ensemble structure built on traditional classifiers and additional image information such as age, gender and nudity estimations. The second model uses a RoBERTa classifier on the text transcriptions to additionally identify the type of problematic ideas the memes perpetuate. Our submissions perform above all organizer submitted baselines. For binary misogyny classification, our system achieved the fifth place on the leaderboard, with a macro F1-score of 0.665. For multi-label classification identifying the type of misogyny, our model achieved place 19 on the leaderboard, with a weighted F1-score of 0.637.},
  doi       = {10.18653/v1/2022.semeval-1.102},
  isbn      = {9781955917803},
  journal   = {SemEval 2022 - 16th International Workshop on Semantic Evaluation, Proceedings of the Workshop},
}

@Article{dongpin2022,
  author    = {Li Dongping and Yang Yingchun and Shen Shikai and He Jun and Shen Haoru and Yue Qiang and Hong Sunyan and Deng Fei},
  journal   = {IAENG International Journal of Computer Science},
  title     = {Research on Deep Learning Model of Multimodal Heterogeneous Data Based on LSTM},
  year      = {2022},
  issn      = {1819-9224},
  volume    = {49},
  abstract  = {To solve the problems of multimodal heterogeneous data fusion and feature learning, a pattern recognition method based on long short-term memory (LSTM) is proposed to improve the classification accuracy. By fusing the deep learning models corresponding to different data types, a shared pattern recognition model is generated based on association analysis. Firstly, classification models for different data type are trained. Long-term memory ability of LSTM is used for the time-dependent data characteristics. Then the fusion strategy is analyzed. And an adaptive determination method for weight fusion is proposed. Finally, the algorithm flow is given, including data preprocessing, model training and heterogeneous data fusing. The experimental results show that the classification accuracy of the proposed method is higher than the models using the single data type alone.},
  issue     = {4},
  keywords  = {Big data,Deep learning,Pattern recognition,Text classification,Time series},
  publisher = {International Association of Engineers},
}

@InProceedings{chenl2022,
  author    = {Lei Chen and Hou Wei Chou},
  booktitle = {Proceedings of The Fifth Workshop on e-Commerce and NLP (ECNLP 5)},
  title     = {Utilizing Cross-Modal Contrastive Learning to Improve Item Categorization BERT Model},
  year      = {2022},
  pages     = {217--223},
  publisher = {Association for Computational Linguistics (ACL)},
  abstract  = {Item categorization (IC) is a core natural language processing (NLP) task in e-commerce. As a special text classification task, fine-tuning pre-trained models, e.g., BERT, has become a main stream solution. To improve IC performance further, other product metadata, e.g., product images, have been used. Although multimodal IC (MIC) systems show higher performance, expanding from processing text to more resource-demanding images brings large engineering impacts and hinders the deployment of such dual-input MIC systems. In this paper, we proposed a new way of using product images to improve text-only IC model: leveraging cross-modal signals between products’ titles and associated images to adapt BERT models in a self-supervised learning (SSL) way. Our experiments on the three genres in the public Amazon product dataset show that the proposed method generates improved prediction accuracy and macro-F1 values than simply using the original BERT. Moreover, the proposed method is able to keep using existing text-only IC inference implementation and shows a resource advantage than the deployment of a dual-input MIC system.},
  doi       = {10.18653/v1/2022.ecnlp-1.25},
  isbn      = {9781955917353},
  journal   = {ECNLP 2022 - 5th Workshop on e-Commerce and NLP, Proceedings of the Workshop},
}

@Article{ortizperez2023,
  author    = {David Ortiz-Perez and Pablo Ruiz-Ponce and David Tomás and Jose Garcia-Rodriguez and M. Flores Vizcaya-Moreno and Marco Leo},
  journal   = {Neurocomputing},
  title     = {A Deep Learning-Based Multimodal Architecture to predict Signs of Dementia},
  year      = {2023},
  issn      = {0925-2312},
  month     = sep,
  pages     = {126413},
  volume    = {548},
  abstract  = {This paper proposes a multimodal deep learning architecture combining text and audio information to predict dementia, a disease which affects around 55 million people all over the world and makes them in some cases dependent people. The system was evaluated on the DementiaBank Pitt Corpus dataset, which includes audio recordings as well as their transcriptions for healthy people and people with dementia. Different models have been used and tested, including Convolutional Neural Networks (CNN) for audio classification, Transformers for text classification, and a combination of both in a multimodal ensemble. These models have been evaluated on a test set, obtaining the best results by using the text modality, achieving 90.36% accuracy on the task of detecting dementia. Additionally, an analysis of the corpus has been conducted for the sake of explainability, aiming to obtain more information about how the models generate their predictions and identify patterns in the data.},
  doi       = {10.1016/j.neucom.2023.126413},
  keywords  = {Deep learning,Dementia prediction,Multimodal,Transformers},
  publisher = {Elsevier BV},
}

@Article{chos2023,
  author    = {Seongkuk Cho and Jihoon Moon and Junhyeok Bae and Jiwon Kang and Sangwook Lee},
  journal   = {Electronics},
  title     = {A Framework for Understanding Unstructured Financial Documents Using RPA and Multimodal Approach},
  year      = {2023},
  issn      = {2079-9292},
  month     = feb,
  number    = {4},
  pages     = {939},
  volume    = {12},
  abstract  = {The financial business process worldwide suffers from huge dependencies upon labor and written documents, thus making it tedious and time-consuming. In order to solve this problem, traditional robotic process automation (RPA) has recently been developed into a hyper-automation solution by combining computer vision (CV) and natural language processing (NLP) methods. These solutions are capable of image analysis, such as key information extraction and document classification. However, they could improve on text-rich document images and require much training data for processing multilingual documents. This study proposes a multimodal approach-based intelligent document processing framework that combines a pre-trained deep learning model with traditional RPA used in banks to automate business processes from real-world financial document images. The proposed framework can perform classification and key information extraction on a small amount of training data and analyze multilingual documents. In order to evaluate the effectiveness of the proposed framework, extensive experiments were conducted using Korean financial document images. The experimental results show the superiority of the multimodal approach for understanding financial documents and demonstrate that adequate labeling can improve performance by up to about 15%.},
  doi       = {10.3390/electronics12040939},
  issue     = {4},
  keywords  = {RPA,financial document analysis,image classification,intelligent document processing,key information extraction,optical character recognition,visual-rich document understanding},
  publisher = {MDPI AG},
}

@InProceedings{fujinumay2023,
  author    = {Fujinuma, Yoshinari and Varia, Siddharth and Sankaran, Nishant and Appalaraju, Srikar and Min, Bonan and Vyas, Yogarshi},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  title     = {A Multi-Modal Multilingual Benchmark for Document Image Classification},
  year      = {2023},
  pages     = {14361--14376},
  publisher = {Association for Computing Machinery (ACM)},
  abstract  = {Document image classification is different from plain-text document classification and consists of classifying a document by understanding the content and structure of documents such as forms, emails, and other such documents. We show that the only existing dataset for this task (Lewis et al., 2006) has several limitations and we introduce two newly curated multilingual datasets (WIKI-DOC and MULTIEURLEXDOC) that overcome these limitations. We further undertake a comprehensive study of popular visually-rich document understanding or Document AI models in previously untested setting in document image classification such as 1) multi-label classification, and 2) zero-shot cross-lingual transfer setup. Experimental results show limitations of multilingual Document AI models on cross-lingual transfer across typologically distant languages. Our datasets and findings open the door for future research into improving Document AI models.},
  doi       = {10.18653/v1/2023.findings-emnlp.958},
  isbn      = {9798891760615},
  journal   = {Findings of the Association for Computational Linguistics: EMNLP 2023},
}

@Article{shah2023,
  author    = {Harsh Shah and Kokil Jaidka and Lyle Ungar and Jesse Fagan and Travis Grosser},
  journal   = {Information},
  title     = {Building a Multimodal Classifier of Email Behavior: Towards a Social Network Understanding of Organizational Communication},
  year      = {2023},
  issn      = {2078-2489},
  month     = dec,
  number    = {12},
  pages     = {661},
  volume    = {14},
  abstract  = {Within organizational settings, communication dynamics are influenced by various factors, such as email content, historical interactions, and interpersonal relationships. We introduce the Email MultiModal Architecture (EMMA) to model these dynamics and predict future communication behavior. EMMA uses data related to an email sender’s social network, performance metrics, and peer endorsements to predict the probability of receiving an email response. Our primary analysis is based on a dataset of 0.6 million corporate emails from 4320 employees between 2012 and 2014. By integrating features that capture a sender’s organizational influence and likability within a multimodal structure, EMMA offers improved performance over models that rely solely on linguistic attributes. Our findings indicate that EMMA enhances email reply prediction accuracy by up to 12.5% compared to leading text-centric models. EMMA also demonstrates high accuracy on other email datasets, reinforcing its utility and generalizability in diverse contexts. Our findings recommend the need for multimodal approaches to better model communication patterns within organizations and teams and to better understand how relationships and histories shape communication trajectories.},
  doi       = {10.3390/info14120661},
  issue     = {12},
  keywords  = {computational linguistics,email,organization,social network analysis,text classification,transformers},
  publisher = {MDPI AG},
}

@Article{rasheeda2023,
  author    = {Assad Rasheed and Arif Iqbal Umar and Syed Hamad Shirazi and Zakir Khan and Muhammad Shahzad},
  journal   = {International Journal on Document Analysis and Recognition (IJDAR)},
  title     = {Cover-based multiple book genre recognition using an improved multimodal network},
  year      = {2022},
  issn      = {1433-2825},
  month     = sep,
  number    = {1},
  pages     = {65--88},
  volume    = {26},
  abstract  = {Despite the idiom not to prejudge something by its outward appearance, we consider deep learning to learn whether we can judge a book by its cover or, more precisely, by its text and design. The classification was accomplished using three strategies, i.e., text only, image only, and both text and image. State-of-the-art CNNs (convolutional neural networks) models were used to classify books through cover images. The Gram and SE layers (squeeze and excitation) were used as an attention unit in them to learn the optimal features and identify characteristics from the cover image. The Gram layer enabled more accurate multi-genre classification than the SE layer. The text-based classification was done using word-based, character-based, and feature engineering-based models. We designed EXplicit interActive Network (EXAN) composed of context-relevant layers and multi-level attention layers to learn features from books title. We designed an improved multimodal fusion architecture for multimodal classification that uses an attention mechanism between modalities. The disparity in modalities convergence speed is addressed by pre-training each sub-network independently prior to end-to-end training of the model. Two book cover datasets were used in this study. Results demonstrated that text-based classifiers are superior to image-based classifiers. The proposed multimodal network outperformed all models for this task with the highest accuracy of 69.09% and 38.12% for Latin and Arabic book cover datasets. Similarly, the proposed EXAN surpassed the extant text classification models by scoring the highest prediction rates of 65.20% and 33.8% for Latin and Arabic book cover datasets.},
  doi       = {10.1007/s10032-022-00413-8},
  issue     = {1},
  keywords  = {Book covers classification,CNN,Image classifiers,Multimodal learning,Text classifiers},
  publisher = {Springer Science and Business Media LLC},
}

@Article{liut2024,
  author    = {Tengfei Liu and Yongli Hu and Junbin Gao and Yanfeng Sun and Baocai Yin},
  journal   = {ACM Transactions on Knowledge Discovery from Data},
  title     = {Cross-modal Multiple Granularity Interactive Fusion Network for Long Document Classification},
  year      = {2024},
  issn      = {1556-472X},
  month     = feb,
  number    = {4},
  pages     = {1--24},
  volume    = {18},
  abstract  = {Long Document Classification (LDC) has attracted great attention in Natural Language Processing and achieved considerable progress owing to the large-scale pre-trained language models. In spite of this, as a different problem from the traditional text classification, LDC is far from being settled. Long documents, such as news and articles, generally have more than thousands of words with complex structures. Moreover, compared with flat text, long documents usually contain multi-modal content of images, which provide rich information but not yet being utilized for classification. In this article, we propose a novel cross-modal method for long document classification, in which multiple granularity feature shifting networks are proposed to integrate the multi-scale text and visual features of long documents adaptively. Additionally, a multi-modal collaborative pooling block is proposed to eliminate redundant fine-grained text features and simultaneously reduce the computational complexity. To verify the effectiveness of the proposed model, we conduct experiments on the Food101 dataset and two constructed multi-modal long document datasets. The experimental results show that the proposed cross-modal method outperforms the single-modal text methods and defeats the state-of-the-art related multi-modal baselines.},
  doi       = {10.1145/3631711},
  issue     = {4},
  keywords  = {Long document classification,cross-modal multi-granularity interactive fusion,multi-modal collaborative pooling},
  publisher = {Association for Computing Machinery (ACM)},
}

@InBook{samya2023,
  author    = {Samy, Ahmed E. and Kefato, Zekarias T. and Girdzijauskas, Šarūnas},
  pages     = {629-636},
  publisher = {IOS Press},
  title     = {Data-Driven Self-Supervised Graph Representation Learning},
  year      = {2023},
  isbn      = {9781643684376},
  month     = sep,
  volume    = {372},
  abstract  = {Self-supervised graph representation learning (SSGRL) is a representation learning paradigm used to reduce or avoid manual labeling. An essential part of SSGRL is graph data augmentation. Existing methods usually rely on heuristics commonly identified through trial and error and are effective only within some application domains. Also, it is not clear why one heuristic is better than another. Moreover, recent studies have argued against some techniques (e.g., dropout: that can change the properties of molecular graphs or destroy relevant signals for graph-based document classification tasks). In this study, we propose a novel data-driven SSGRL approach that automatically learns a suitable graph augmentation from the signal encoded in the graph (i.e., the nodes' predictive feature and topological information). We propose two complementary approaches that produce learnable feature and topological augmentations. The former learns multi-view augmentation of node features, and the latter learns a high-order view of the topology. Moreover, the augmentations are jointly learned with the representation. Our approach is general that it can be applied to homogeneous and heterogeneous graphs. We perform extensive experiments on node classification (using nine homogeneous and heterogeneous datasets) and graph property prediction (using another eight datasets). The results show that the proposed method matches or outperforms the SOTA SSGRL baselines and performs similarly to semi-supervised methods. The anonymised source code is available at https://github.com/AhmedESamy/dsgrl/},
  booktitle = {ECAI 2023},
  doi       = {10.3233/faia230325},
  issn      = {1879-8314},
  journal   = {Frontiers in Artificial Intelligence and Applications},
}

@Article{jarrahia2023,
  author    = {Ali Jarrahi and Leila Safari},
  journal   = {Multimedia Tools and Applications},
  title     = {Evaluating the effectiveness of publishers’ features in fake news detection on social media},
  year      = {2022},
  issn      = {1573-7721},
  month     = apr,
  number    = {2},
  pages     = {2913--2939},
  volume    = {82},
  abstract  = {With the expansion of the Internet and attractive social media infrastructures, people prefer to follow the news through these media. Despite the many advantages of these media in the news field, the lack of control and verification mechanism has led to the spread of fake news as one of the most critical threats to democracy, economy, journalism, health, and freedom of expression. So, designing and using efficient automated methods to detect fake news on social media has become a significant challenge. One of the most relevant entities in determining the authenticity of a news statement on social media is its publishers. This paper examines the publishers’ features in detecting fake news on social media, including Credibility, Influence, Sociality, Validity, and Lifetime. In this regard, we propose an algorithm, namely CreditRank, for evaluating publishers’ credibility on social networks. We also suggest a high accurate multi-modal framework, namely FR-Detect, for fake news detection using user-related and content-related features. Furthermore, a sentence-level convolutional neural network is provided to properly combine publishers’ features with latent textual content features. Experimental results show that the publishers’ features can improve the performance of content-based models by up to 16% and 31% in accuracy and F1, respectively. Also, the behavior of publishers in different news domains has been statistically studied and analyzed.},
  doi       = {10.1007/s11042-022-12668-8},
  issue     = {2},
  keywords  = {CreditRank algorithm,Deep neural network,Fake news detection,Machine learning,Social media,Text classification},
  publisher = {Springer Science and Business Media LLC},
}

@Article{liangz2023,
  author    = {Zhiping Liang},
  journal   = {Computers, Materials &amp; Continua},
  title     = {Fake News Detection Based on Multimodal Inputs},
  year      = {2023},
  issn      = {1546-2226},
  number    = {2},
  pages     = {4519--4534},
  volume    = {75},
  abstract  = {In view of the various adverse effects, fake news detection has become an extremely important task. So far, many detection methods have been proposed, but these methods still have some limitations. For example, only two independently encoded unimodal information are concatenated together, but not integrated with multimodal information to complete the complementary information, and to obtain the correlated information in the news content. This simple fusion approach may lead to the omission of some information and bring some interference to the model. To solve the above problems, this paper proposes the Fake News Detection model based on BLIP (FNDB). First, the XLNet and VGG-19 based feature extractors are used to extract textual and visual feature representation respectively, and BLIP based multimodal feature extractor to obtain multimodal feature representation in news content. Then, the feature fusion layer will fuse these features with the help of the cross-modal attention module to promote various modal feature representations for information complementation. The fake news detector uses these fused features to identify the input content, and finally complete fake news detection. Based on this design, FNDB can extract as much information as possible from the news content and fuse the information between multiple modalities effectively. The fake news detector in the FNDB can also learn more information to achieve better performance. The verification experiments on Weibo and Gossipcop, two widely used real-world datasets, show that FNDB is 4.4% and 0.6% higher in accuracy than the state-of-the-art fake news detection methods, respectively.},
  doi       = {10.32604/cmc.2023.037035},
  issue     = {2},
  keywords  = {Natural language processing,fake news detection,machine learning,text classification},
  publisher = {Tech Science Press},
}

@Article{kozienkop2023,
  author    = {Przemysław Kazienko and Julita Bielaniewicz and Marcin Gruza and Kamil Kanclerz and Konrad Karanowski and Piotr Miłkowski and Jan Kocoń},
  journal   = {Information Fusion},
  title     = {Human-centered neural reasoning for subjective content processing: Hate speech, emotions, and humor},
  year      = {2023},
  issn      = {1566-2535},
  month     = jun,
  pages     = {43--65},
  volume    = {94},
  abstract  = {Some tasks in content processing, e.g., natural language processing (NLP), like hate or offensive speech and emotional or funny text detection, are subjective by nature. Each human may perceive some content individually. The existing reasoning methods commonly rely on agreed output values, the same for all recipients. We propose fundamentally different — personalized solutions applicable to any subjective NLP task. Our five new deep learning models take into account not only the textual content but also the opinions and beliefs of a given person. They differ in their approaches to learning Human Bias (HuBi) and fusion with content (text) representation. The experiments were carried out on 14 tasks related to offensive, emotional, and humorous texts. Our personalized HuBi methods radically outperformed the generalized ones for all NLP problems. Personalization also has a greater impact on reasoning quality than commonly explored pre-trained and fine-tuned language models. We discovered a high correlation between human bias calculated using our dedicated formula and that learned by the model. Multi-task solutions achieved better outcomes than single-task architectures. Human and word embeddings also provided additional insights.},
  doi       = {10.1016/j.inffus.2023.01.010},
  keywords  = {Content perception,Emotion recognition,Hate speech,Human bias,Humor detection,Information fusion,Learning human representations,NLP,Offensive content,Personalized NLP,Subjective NLP tasks,Text classification},
  publisher = {Elsevier BV},
}

@InProceedings{graffm2023,
  author    = {Mario Graff and Daniela Moctezuma and Eric Tellez and Sabino Miranda},
  title     = {Ingeotec at DA-VINCIS: Bag-of-Words Classifiers},
  year      = {2023},
  publisher = {CEUR-WS},
  volume    = {3496},
  abstract  = {Violence is a serious problem that can have a devastating impact on individuals and communities. In some cases, the virtual world is prone to aggressive expressions and insults, among other violent expressions. Also, social media platforms provide a valuable source of information for detecting and monitoring violent events, as people often share posts about them in real time. This information can be used to improve responses to violence in the world and to design better crime prevention policies. In this sense, the DA-VINCIS task at IberLEF 2023 is a competition to develop multimodal models to detect violent incidents on Twitter. The task has two tracks: i) violent event identification and ii) violent event category recognition. This manuscript describes our solution system for the (i) track using only text-based features. More particularly, we used EvoMSA, our multilingual text classification framework based on stacked generalization, to develop competitive models against more complex approaches achieving an F1 score of 0.89, just 3 hundredths behind the best model in the final ranking.},
  issn      = {1613-0073},
  journal   = {CEUR Workshop Proceedings},
  keywords  = {Identification of violent events,Models based on a bag of words,Pre-trained vocabularies,Stacked generalization},
}

@Article{varmanp2023,
  author    = {Pawan Kumar Verma and Prateek Agrawal and Vishu Madaan and Radu Prodan},
  journal   = {Journal of Ambient Intelligence and Humanized Computing},
  title     = {MCred: multi-modal message credibility for fake news detection using BERT and CNN},
  year      = {2022},
  issn      = {1868-5145},
  month     = jul,
  number    = {8},
  pages     = {10617--10629},
  volume    = {14},
  abstract  = {Online social media enables low cost, easy access, rapid propagation, and easy communication of information, including spreading low-quality fake news. Fake news has become a huge threat to every sector in society, and resulting in decrements in the trust quotient for media and leading the audience into bewilderment. In this paper, we proposed a new framework called Message Credibility (MCred) for fake news detection that utilizes the benefits of local and global text semantics. This framework is the fusion of Bidirectional Encoder Representations from Transformers (BERT) using the relationship between words in sentences for global text semantics, and Convolutional Neural Networks (CNN) using N-gram features for local text semantics. We demonstrate through experimental results a popular Kaggle dataset that MCred improves the accuracy over a state-of-the-art model by 1.10% thanks to its combination of local and global text semantics.},
  doi       = {10.1007/s12652-022-04338-2},
  issue     = {8},
  keywords  = {Convolutional neural network,Deep learning,Dense network,Fake news classification,Global semantic,Natural language processing,Social media disinformation,Text classification,local semantic},
  publisher = {Springer Science and Business Media LLC},
}

@InProceedings{tianl2023,
  author     = {Lin Tian and Xiuzhen Zhang and Jey Han Lau},
  booktitle  = {Proceedings of the ACM Web Conference 2023},
  title      = {MetaTroll: Few-shot Detection of State-Sponsored Trolls with Transformer Adapters},
  year       = {2023},
  month      = apr,
  pages      = {1743-1753},
  publisher  = {Association for Computing Machinery (ACM)},
  series     = {WWW ’23},
  abstract   = {State-sponsored trolls are the main actors of influence campaigns on social media and automatic troll detection is important to combat misinformation at scale. Existing troll detection models are developed based on training data for known campaigns (e.g. the influence campaign by Russia's Internet Research Agency on the 2016 US Election), and they fall short when dealing with novel campaigns with new targets. We propose MetaTroll, a text-based troll detection model based on the meta-learning framework that enables high portability and parameter-efficient adaptation to new campaigns using only a handful of labelled samples for few-shot transfer. We introduce campaign-specific transformer adapters to MetaTroll to "memorise"campaign-specific knowledge so as to tackle catastrophic forgetting, where a model "forgets"how to detect trolls from older campaigns due to continual adaptation. Our experiments demonstrate that MetaTroll substantially outperforms baselines and state-of-the-art few-shot text classification models. Lastly, we explore simple approaches to extend MetaTroll to multilingual and multimodal detection. Source code for MetaTroll is available at: https://github.com/ltian678/metatroll-code.git},
  collection = {WWW ’23},
  doi        = {10.1145/3543507.3583417},
  isbn       = {9781450394161},
  journal    = {ACM Web Conference 2023 - Proceedings of the World Wide Web Conference, WWW 2023},
  keywords   = {adapter,continual learning,few-shot learning,multilingual,multimodal,troll detection},
}

@Article{kenny2023,
  author    = {Kenny and Andry Chowanda},
  journal   = {ICIC Express Letters},
  title     = {MULTIMODAL APPROACH FOR EMOTION RECOGNITION USING FEATURE FUSION},
  year      = {2023},
  issn      = {1881-803X},
  month     = {2},
  pages     = {181-189},
  volume    = {17},
  abstract  = {Emotion recognition has been a challenge. Multimodality approach in emotion classification has been used in many research to improve the recognition performance. Nevertheless, there is a lack of understanding between how the multimodality affects the performance of the model. This paper uses IEMOCAP as dataset and creates several unimodal model and multimodal model resulting in combination of the top unimodal model for emotion recognition with feature fusion method which merges features from different models. After evaluating the models, this paper analyzes the connection of every unimodality involved and its implication to multimodality built. This paper also applies audio augmentation to reducing overfitting in model’s prediction. The top result of multimodal model consisting of 3 modalities achieves F1 score of 71.25% and the model consisting of 2 modalities achieves F1 score of 76.5%.},
  doi       = {10.24507/icicel.17.02.181},
  issue     = {2},
  keywords  = {Audio augmentations,Audio classification,Deep learning,Emotion recognition,Image classification,Multimodal classification,Text classification},
  publisher = {ICIC International},
}

@InProceedings{linckere2023,
  author     = {Elise Lincker and Camille Guinaudeau and Olivier Pons and Jérôme Dupire and Céline Hudelot and Vincent Mousseau and Isabelle Barbet and Caroline Huron},
  booktitle  = {20th International Conference on Content-based Multimedia Indexing},
  title      = {Noisy and Unbalanced Multimodal Document Classification: Textbook Exercises as a Use Case},
  year       = {2023},
  month      = sep,
  pages      = {71--78},
  publisher  = {Association for Computing Machinery (ACM)},
  series     = {CBMI 2023},
  abstract   = {In order to foster inclusive education, automatic systems that can adapt textbooks to make them accessible to children with Developmental Coordination Disorder (DCD) are necessary. In this context, we propose a task to classify exercises according to their DCD adaptation type. We introduce a challenging exercise dataset extracted from French textbooks, with two major difficulties: limited and unbalanced, noisy data. To set a baseline on the dataset, we use state-of-the-art models combined through early and late fusion techniques to take advantage of text and vision/layout modalities. Our approach achieves an overall accuracy of 0.802. However, the experiments show the difficulty of the task, especially for minority classes, where the accuracy drops to 0.583.},
  collection = {CBMI 2023},
  doi        = {10.1145/3617233.3617239},
  isbn       = {9798400709128},
  journal    = {ACM International Conference Proceeding Series},
  keywords   = {multimodal document classification,noisy data,textbook adaptation,unbalanced data},
}

@Article{jarquin2023,
  author    = {Horacio Jarquín-Vásquez and Delia Irazú Hernández-Farías and Luis Joaquín Arellano and Hugo Jair Escalante and Luis Villaseñor-Pineda and Manuel Montes-y-Gómez and Fernando Sanchez-Vega},
  journal   = {Procesamiento del Lenguaje Natural},
  title     = {Overview of DA-VINCIS at IberLEF 2023: Detection of Aggressive and Violent Incidents from Social Media in Spanish},
  year      = {2023},
  issn      = {1989-7553},
  month     = {9},
  pages     = {351–360},
  abstract  = {In this paper, we present the overview of the DA-VINCIS 2023 shared task which was organized at IberLEF 2023 and co-located in the framework of the 39th International Conference of the Spanish Society for Natural Language Processing (SEPLN 2023). The main aim of this task is to promote the research on developing automatic solutions for detecting violent events in social networks. Two subtasks were considered: (i) A binary classification task aimed to determine whether or not a tweet is about a violent incident; and (ii) A multi-label multi-class classification task in which the category(ies) of a violent incident must be identified. A multimodal manual annotated corpus comprising both tweets and images associated to them was provided to the participants. A total of 15 systems were submitted for the final evaluation phase. Competitive results were obtained for both subtasks, the higher ones were in the binary classification task. Corpora and results are available at the shared task website at https://codalab.lisn.upsaclay.fr/competitions/11312.},
  doi       = {10.26342/2023-71-27},
  issue     = {71},
  keywords  = {DA-VINCIS,text classification,violent event detection},
  publisher = {Sociedad Española para el Procesamiento del Lenguaje Natural},
}

@Article{luzdearau2023,
  author    = {Luz de Araujo, Pedro H. and de Almeida, Ana Paula G. S. and Ataides Braz, Fabricio and Correia da Silva, Nilton and de Barros Vidal, Flavio and de Campos, Teofilo E.},
  journal   = {International Journal on Document Analysis and Recognition (IJDAR)},
  title     = {Sequence-aware multimodal page classification of Brazilian legal documents},
  year      = {2022},
  issn      = {1433-2825},
  month     = jul,
  number    = {1},
  pages     = {33--49},
  volume    = {26},
  abstract  = {The Brazilian Supreme Court receives tens of thousands of cases each semester. Court employees spend thousands of hours to execute the initial analysis and classification of those cases—which takes effort away from posterior, more complex stages of the case management workflow. In this paper, we explore multimodal classification of documents from Brazil’s Supreme Court. We train and evaluate our methods on a novel multimodal dataset of 6510 lawsuits (339,478 pages) with manual annotation assigning each page to one of six classes. Each lawsuit is an ordered sequence of pages, which are stored both as an image and as a corresponding text extracted through optical character recognition. We first train two unimodal classifiers: A ResNet pre-trained on ImageNet is fine-tuned on the images, and a convolutional network with filters of multiple kernel sizes is trained from scratch on document texts. We use them as extractors of visual and textual features, which are then combined through our proposed fusion module. Our fusion module can handle missing textual or visual input by using learned embeddings for missing data. Moreover, we experiment with bidirectional long short-term memory (biLSTM) networks and linear-chain conditional random fields to model the sequential nature of the pages. The multimodal approaches outperform both textual and visual classifiers, especially when leveraging the sequential nature of the pages.},
  doi       = {10.1007/s10032-022-00406-7},
  issue     = {1},
  keywords  = {Document classification,Legal domain,Multimodal page classification,Portuguese language processing,Sequence classification},
  publisher = {Springer Science and Business Media LLC},
}

@InProceedings{chenz2023,
  author    = {Zhihong Chen and Shizhe Diao and Benyou Wang and Guanbin Li and Xiang Wan},
  booktitle = {2023 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts},
  year      = {2023},
  month     = oct,
  pages     = {23346--23356},
  publisher = {IEEE},
  abstract  = {Medical vision-and-language pre-training (Med-VLP) has shown promising improvements on many downstream medical tasks owing to its applicability to extracting generic representations from medical images and texts. Practically, there exist two typical types, i.e., the fusion-encoder type and the dual-encoder type, depending on whether a heavy fusion module is used. The former is superior at multi-modal tasks owing to the sufficient interaction between modalities; the latter is good at uni-modal and cross-modal tasks due to the single-modality encoding ability. To take advantage of these two types, we propose an effective yet straightforward scheme named PTUnifier to unify the two types. We first unify the input format by introducing visual and textual prompts, which serve as DETR-like queries that assist in extracting features when one of the modalities is missing. By doing so, a single model could serve as a foundation model that processes various tasks adopting different input formats (i.e., image-only, text-only, and image-text-pair). Furthermore, we construct a prompt pool (instead of static ones) to improve diversity and scalability, enabling queries conditioned on different input instances. Experimental results show that our approach achieves competitive results on a broad range of tasks, spanning uni-modal tasks (i.e., image/text classification and text summarization), cross-modal tasks (i.e., image-to-text generation and image-text/text-image retrieval), and multi-modal tasks (i.e., visual question answering), demonstrating the effectiveness of our approach. Note that the adoption of prompts is orthogonal to most existing Med-VLP approaches and could be a beneficial and complementary extension to these approaches. The source code is available at https://github.com/zhjohnchan/ptunifier.},
  doi       = {10.1109/iccv51070.2023.02139},
  isbn      = {9798350307184},
  issn      = {1550-5499},
  journal   = {Proceedings of the IEEE International Conference on Computer Vision},
}

@InProceedings{zouh2023,
  author    = {Heqing Zou and Meng Shen and Chen Chen and Yuchen Hu and Deepu Rajan and Eng Siong Chng},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  title     = {UniS-MMC: Multimodal Classification via Unimodality-supervised Multimodal Contrastive Learning},
  year      = {2023},
  pages     = {659-672},
  publisher = {Association for Computational Linguistics (ACL)},
  abstract  = {Multimodal learning aims to imitate human beings to acquire complementary information from multiple modalities for various downstream tasks. However, traditional aggregation-based multimodal fusion methods ignore the inter-modality relationship, treat each modality equally, suffer sensor noise, and thus reduce multimodal learning performance. In this work, we propose a novel multimodal contrastive method to explore more reliable multimodal representations under the weak supervision of unimodal predicting. Specifically, we first capture task-related unimodal representations and the unimodal predictions from the introduced unimodal predicting task. Then the unimodal representations are aligned with the more effective one by the designed multimodal contrastive method under the supervision of the unimodal predictions. Experimental results with fused features on two image-text classification benchmarks UPMC-Food-101 and N24News show that our proposed Unimodality-Supervised MultiModal Contrastive (UniS-MMC) learning method outperforms current state-of-the-art multimodal methods. The detailed ablation study and analysis further demonstrate the advantage of our proposed method.},
  doi       = {10.18653/v1/2023.findings-acl.41},
  isbn      = {9781959429623},
  issn      = {0736-587X},
  journal   = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
}

@Article{bakkalis2023,
  author    = {Souhail Bakkali and Zuheng Ming and Mickael Coustaty and Marçal Rusiñol and Oriol Ramos Terrades},
  journal   = {Pattern Recognition},
  title     = {VLCDoC: Vision-Language contrastive pre-training model for cross-Modal document classification},
  year      = {2023},
  issn      = {0031-3203},
  month     = jul,
  pages     = {109419},
  volume    = {139},
  abstract  = {Multimodal learning from document data has achieved great success lately as it allows to pre-train semantically meaningful features as a prior into a learnable downstream task. In this paper, we approach the document classification problem by learning cross-modal representations through language and vision cues, considering intra- and inter-modality relationships. Instead of merging features from different modalities into a joint representation space, the proposed method exploits high-level interactions and learns relevant semantic information from effective attention flows within and across modalities. The proposed learning objective is devised between intra- and inter-modality alignment tasks, where the similarity distribution per task is computed by contracting positive sample pairs while simultaneously contrasting negative ones in the joint representation space. Extensive experiments on public benchmark datasets demonstrate the effectiveness and the generality of our model both on low-scale and large-scale datasets.},
  doi       = {10.1016/j.patcog.2023.109419},
  keywords  = {Contrastive learning,Document classification,Multimodal document representation learning,Self-Attention,Transformers},
  publisher = {Elsevier BV},
}

@Article{wajdm2024,
  author    = {Mohd Anas Wajid and Aasim Zafar and Mohammad Saif Wajid},
  journal   = {International Journal of Information Technology},
  title     = {A deep learning approach for image and text classification using neutrosophy},
  year      = {2023},
  issn      = {2511-2112},
  month     = oct,
  number    = {2},
  pages     = {853--859},
  volume    = {16},
  abstract  = {The proliferation of data on the web and on personal computers is a direct result of the proliferation of new technologies and gadgets. Most of these pieces of information are collected through a number of different methods (text, image, video, etc.). This type of information is also vital for e-commerce websites. The products on these websites feature both images and descriptions in text form, making them multimodal in nature. Earlier classification and information retrieval algorithms focused largely on a single modality. This study leverages multimodal data for categorization utilising neutrosophic fuzzy sets for uncertainty management for information retrieval tasks. This work employs image and text data and, inspired by prior ways of embedding text over an image, seeks to classify the images using neutrosophic classification algorithms. Neurosophic convolutional neural networks (NCNNs) are used to learn feature representations of the generated images for classification tasks. We demonstrate how a pipeline based on NCNN may be applied to learn representations of the unique fusion method. Traditional convolutional neural networks are subject to unexpected noisy conditions in the test phase, and as a result, their performance for the categorization of noisy data degrades. Comparing our technique against individual sources on multi-modal classification dataset provides good results.},
  doi       = {10.1007/s41870-023-01529-8},
  issue     = {2},
  keywords  = {Convolutional neutral network,Early fusion,Fuzzy logic,Information retrieval,Late fusion,Multimodal data,Neutrosophic logic},
  publisher = {Springer Science and Business Media LLC},
}

@Article{fengz2024,
  author    = {Zijian Feng and Kezhi Mao and Hanzhang Zhou},
  journal   = {Expert Systems with Applications},
  title     = {Adaptive micro- and macro-knowledge incorporation for hierarchical text classification},
  year      = {2024},
  issn      = {0957-4174},
  month     = aug,
  pages     = {123374},
  volume    = {248},
  abstract  = {Hierarchical text classification (HTC) aims to classify a text into multiple categories organized in a hierarchical structure. The state-of-the-art HTC methods usually employ graph networks, where label graphs are constructed and label representation is learned to interact with text representations for classification. In general, label graphs are built on the intrinsic label hierarchy, label semantic similarity, or label co-occurrence. Such graphs have been proven to be effective, but they only exploit knowledge from training data or simple label descriptions, without considering the vast external knowledge in the open sources. Actually, external knowledge from open sources could bring in complementary information to enhance the label graph's representation power. Motivated by the above considerations, we explore the use of external knowledge for improving HTC in this paper. We categorize knowledge into micro-knowledge and macro-knowledge, which are defined as the fundamental concepts related to a single class label and the correlations among class labels, respectively. For tailor-made incorporation of the two types of knowledge into representation learning and classification, we propose Adaptive Micro- and Macro-Knowledge Incorporation for Hierarchical Text Classification (AMKI-HTC) model in this paper. The micro-knowledge incorporation helps capture class-relevant keywords in the text and hence produce discriminative representations, while the macro-knowledge incorporation improves the accuracy of label graphs. Finally, a confidence maximization fusion strategy is developed for adaptive aggregation of multi-view features. Extensive experiments on three benchmark HTC datasets demonstrate that AMKI-HTC consistently outperforms state-of-the-art models.},
  doi       = {10.1016/j.eswa.2024.123374},
  keywords  = {Adaptive fusion,Hierarchical text classification,Knowledge incorporation},
  publisher = {Elsevier BV},
}

@Article{xuy2024,
  author    = {Yangshuyi Xu and Guangzhong Liu and Lin Zhang and Xiang Shen and Sizhe Luo},
  journal   = {Artificial Intelligence Review},
  title     = {An effective multi-modal adaptive contextual feature information fusion method for Chinese long text classification},
  year      = {2024},
  issn      = {1573-7462},
  month     = aug,
  number    = {9},
  pages     = {1-29},
  volume    = {57},
  abstract  = {Chinese long text classification plays a vital role in Natural Language Processing. Compared to Chinese short texts, Chinese long texts contain more complex semantic feature information. Furthermore, the distribution of these semantic features is uneven due to the varying lengths of the texts. Current research on Chinese long text classification models primarily focuses on enhancing text semantic features and representing Chinese long texts as graph-structured data. Nonetheless, these methods are still susceptible to noise information and tend to overlook the deep semantic information in long texts. To address the above challenges, this study proposes a novel and effective method called MACFM, which introduces a deep feature information mining method and an adaptive modal feature information fusion strategy to learn the semantic features of Chinese long texts thoroughly. First, we present the DCAM module to capture complex semantic features in Chinese long texts, allowing the model to learn detailed high-level representation features. Then, we explore the relationships between word vectors and text graphs, enabling the model to capture abundant semantic information and text positional information from the graph. Finally, we develop the AMFM module to effectively combine different modal feature representations and eliminate the unrelated noise information. The experimental results on five Chinese long text datasets show that our method significantly improves the accuracy of Chinese long text classification tasks. Furthermore, the generalization experiments on five English datasets and the visualized results demonstrate the effectiveness and interpretability of the MACFM model.},
  doi       = {10.1007/s10462-024-10835-x},
  issue     = {9},
  keywords  = {Adaptive modal feature fusion,Chinese long text classification,Graph convolutional network,Modal interaction,Noise information filtering},
  publisher = {Springer Science and Business Media LLC},
}

@Article{jiz2024,
  author    = {Zhenyan Ji and Deyan Kong and Yanyan Yang and Jiqiang Liu and Zhao Li},
  journal   = {Knowledge-Based Systems},
  title     = {ASSL-HGAT: Active semi-supervised learning empowered heterogeneous graph attention network},
  year      = {2024},
  issn      = {0950-7051},
  month     = apr,
  pages     = {111567},
  volume    = {290},
  abstract  = {Recently, heterogeneous graph attention network (HGAT) has been widely applied to various machine learning tasks and achieved remarkable results with sufficient labeled data. However, it is noteworthy that in many tasks, labeled data is scarce and the data labeling process is expensive. To that end, this paper presents a novel framework for learning from data with limited labels by organically integrating active learning (AL) and semi-supervised learning (SSL) into heterogeneous graph network. Our framework consists of three components: heterogeneous information network (HIN), HGAT and active semi-supervised learning strategy (ASSL). The adjustable HIN is first constructed by fusing multi-modal features. Then the HGAT is used to encode HIN and the double-layer attention mechanism can mitigate noises during information fusion. The ASSL is further designed to enrich the high-quality labeled training data for model train. Samples selected by uncertainty-aware AL and samples labeled by pseudo-label selection based SSL are finally mixed to iteratively train the ASSL-HGAT framework. Experimental results prove that ASSL-HGAT surpasses the compared state-of-the-art methods on five representative semi-supervised benchmark datasets under the same experimental settings.},
  doi       = {10.1016/j.knosys.2024.111567},
  keywords  = {Active learning,Graph attention network,Negative sample learning,Pre-trained model,Semi-supervised learning,Short text classification},
  publisher = {Elsevier BV},
}

@Article{ghorbanali2024,
  author    = {Alireza Ghorbanali and Mohammad Karim Sohrabi},
  journal   = {Expert Systems with Applications},
  title     = {Capsule network-based deep ensemble transfer learning for multimodal sentiment analysis},
  year      = {2024},
  issn      = {0957-4174},
  month     = apr,
  pages     = {122454},
  volume    = {239},
  abstract  = {Understanding the attitudes of users about different topics through their huge amount of comments and opinions on social networks is an emerging hot issue. Analyzing the sentiments of these posts and comments provides useful information utilized for many applications. Multimodal sentiments often contain more information than single-modal sentiments, including text, image, audio and video, which leads to better performance of multimodal sentiment analysis (MSA) compared to single-modal sentiment analysis (SSA). In this paper, a capsule network-based deep ensemble transfer learning approach, called DSY-ETL-MSA, is proposed for MSA on images and texts, the results of which are fused using Yager theory. Capsule networks and ensemble learning methods improve classification performance, and the deep transfer learning approach reduces the training time. A hybrid deep architecture is used for automatic feature extraction in the proposed method. For analyzing the sentiment of image modality, a pre-trained VGG16 model, fine-tuned on the datasets, is used to extract high-level features for image classification. A capsule convolutional neural network (CNN) is also separately used for extracting image features and classifying them. In the text modality, the pre-trained GloVe model is exploited to embed words and 2 separate classifiers are employed for text classifications. The Yager fusion rules are finally used for early and late fusions of the results of the classifiers. The results of the text and image classifiers are combined separately as the early fusion and the final results of them are fused at the decision level as the late fusion. The proposed model is empirically evaluated on the MVSA and T4SA datasets. The significant performance improvement compared to a variety of former methods is shown in the experimental results. The final accuracies obtained by the proposed method on the MVSA and T4SA datasets are 0.9866 and 0.9996, respectively.},
  doi       = {10.1016/j.eswa.2023.122454},
  keywords  = {Capsule network,Ensemble,Evidence theory,Multimodal sentiment analysis,Transfer learning},
  publisher = {Elsevier BV},
}

@Article{yushili2024,
  author    = {Yushi Li and Xin Zheng and Ming Zhu and Jie Mei and Ziwen Chen and Yunfei Tao},
  journal   = {Signal, Image and Video Processing},
  title     = {Compact bilinear pooling and multi-loss network for social media multimodal classification},
  year      = {2024},
  issn      = {1863-1711},
  month     = aug,
  number    = {11},
  pages     = {8403--8412},
  volume    = {18},
  abstract  = {Social media platforms have seen an influx of multimodal data, leading to heightened attention on image-text multimodal classification. Existing methods for multimodal classification primarily focus on multimodal fusion from different modalities. However, owing to the heterogeneity and high-dimensionality of multimodal data, the fusion process frequently introduces redundant information and noise limiting the accuracy and generalization. To resolve the limitation, we propose a Compact Bilinear pooling and Multi-Loss network (CBMLNet). Compact bilinear pooling is used for feature fusion to learn low-dimensional and expressive multimodal representations efficiently. Furthermore, a multi-loss function is proposed to import the specific information carried by each single modality. Therefore, CBMLNet simultaneously considers the correlation between multimodality and the specificity of single modality for image-text classification. We evaluate the proposed CBMLNet on two publicly available datasets, Twitter-15 and Twitter-17, and on a private dataset, AIFUN. CBMLNet is compared with the advanced methods such as multimodal BERT with Max Pooling, Multi-Interactive Memory Network, Multi-level Multi-modal Cross-attention Network, Image-Text Correlation model (ITC), Target-oriented multimodal BERT and multimodal hierarchical attention model (MHA). Experimental results demonstrate that CBMLNet averagely improves F1_score by 0.28% and 0.44% compared with the best fine-grained baseline, MHA and the best coarse-grained baseline, ITC. It illustrates that CBMLNet is practical for real-world fuzzy applications as a coarse-grained model.},
  doi       = {10.1007/s11760-024-03482-w},
  issue     = {11},
  keywords  = {Compact bilinear pooling,Feature fusion,Multi-loss,Multimodal classification,Social media},
  publisher = {Springer Science and Business Media LLC},
  venue     = {Signal, Image and Video Processing},
}

@Article{zhangqi2024,
  author    = {Zhangqi Jiang and Tingjin Luo and Xinyan Liang},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title     = {Deep Incomplete Multi-View Learning Network with Insufficient Label Information},
  year      = {2024},
  issn      = {2159-5399},
  month     = mar,
  number    = {11},
  pages     = {12919--12927},
  volume    = {38},
  abstract  = {Due to the efficiency of integrating semantic consensus and complementary information across different views, multi-view classification methods have attracted much attention in recent years. However, multi-view data often suffers from both the miss of view features and insufficient label information, which significantly decrease the performance of traditional multi-view classification methods in practice. Learning for such simultaneous lack of feature and label is crucial but rarely studied. To tackle these problems, we propose a novel Deep Incomplete Multi-view Learning Network (DIMvLN) by incorporating graph networks and semi-supervised learning in this paper. Specifically, DIMvLN firstly designs the deep graph networks to effectively recover missing data with assigning pseudo-labels of large amounts of unlabeled instances and refine the incomplete feature information. Meanwhile, to enhance the label information, a novel pseudo-label generation strategy with the similarity constraints of unlabeled instances is proposed to exploit additional supervisory information and guide the completion module to preserve more semantic information of absent multi-view data. Besides, we design view-specific representation extractors with the autoencoder structure and contrastive loss to learn high-level semantic representations for each view, promote cross-view consistencies and augment the separability between different categories. Finally, extensive experimental results demonstrate the effectiveness of our DIMvLN, attaining noteworthy performance improvements compared to state-of-the-art competitors on several public benchmark datasets. Code will be available at GitHub.},
  doi       = {10.1609/aaai.v38i11.29189},
  isbn      = {1577358872},
  issue     = {11},
  keywords  = {ML: Classification and Regression,ML: Multimodal Learning,ML: Semi,Supervised Learning,class/Multi,label Learning & Extreme Classification,view Learning},
  publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/29189},
}

@Article{xianfang2024,
  author    = {Xianfang Song and Yong Zhang and Wanqiu Zhang and Chunlin He and Ying Hu and Jian Wang and Dunwei Gong},
  journal   = {Swarm and Evolutionary Computation},
  title     = {Evolutionary computation for feature selection in classification: A comprehensive survey of solutions, applications and challenges},
  year      = {2024},
  issn      = {2210-6502},
  month     = oct,
  pages     = {101661},
  volume    = {90},
  abstract  = {Feature selection (FS), as one of the most significant preprocessing techniques in the fields of machine learning and pattern recognition, has received great attention. In recent years, evolutionary computation has become a popular technique for handling FS problems due to its superior global search performance. In this paper, a comprehensive review of evolutionary computation research on the FS problems is presented. Firstly, a new taxonomy for the basic components of evolutionary feature selection algorithms (EFSs) is proposed, including encoding strategy, population initialization, population updating, local search, multi-FS hybrid and ensemble. Secondly, we summarize the latest research progress of EFSs on some new and complex scenarios, including large-scale high-dimensional data, multi-objective/metric scenario, multi-label data, distributed storage data, multi-task scenario, multi-modal scenario, interpretable FS and stable FS, etc. Moreover, this survey provides also an in-depth analysis of real-world applications of EFSs, such as hyperspectral band selection, bioinformatics gene selection, text classification and fault detection, etc. Finally, several opportunities for future work are pointed out.},
  doi       = {10.1016/j.swevo.2024.101661},
  keywords  = {Evolutionary computation,Feature selection,Particle swarm optimization,Swarm intelligence},
  publisher = {Elsevier BV},
}

@InBook{zhangy2024,
  author    = {Yin Zhang},
  pages     = {undefined-undefined},
  publisher = {IOS Press},
  title     = {Image Information Prompt: Tips for Learning Large Language Models},
  year      = {2024},
  isbn      = {9781643684871},
  month     = jan,
  abstract  = {Sentiment text classification is a natural language processing technique for recognizing and extracting subjective information in text, such as emotions, attitudes, and opinions. Traditional models such as BERT and XLNet have achieved brilliant results in text classification problems. With the rise of large language model technology, using generative large models to improve text classification accuracy has become a new research direction. However, compared with traditional classification models, large language models have a slight disadvantage in language classification tasks. In this paper, we propose a prompt-based approach to enhance the accuracy of large language models for text classification using image prompt information on multimodal datasets. First, we illustrate the principle of consistency between image and textual information. Second, we propose a multimodal framework for sentiment analysis of images and text, which realizes the prediction of sentiment tendency for both image and textual data by injecting the image prompt information into the text and into the Large Language Model. Finally, we designed experiments and evaluated them using real multimodal datasets to verify the effectiveness and accuracy of the framework.},
  booktitle = {Intelligent Computing Technology and Automation},
  doi       = {10.3233/atde231197},
  issn      = {2352-7528},
  url       = {https://www.mendeley.com/catalogue/cb65323d-510b-31b5-a865-34cc24fff44c/},
}

@Article{liub2024,
  author    = {Bo Liu and Lejian He and Yuchen Xie and Yuejia Xiang and Li Zhu and Weiping Ding},
  journal   = {Information Fusion},
  title     = {MinJoT: Multimodal infusion Joint Training for noise learning in text and multimodal classification problems},
  year      = {2024},
  issn      = {1566-2535},
  month     = feb,
  pages     = {102071},
  volume    = {102},
  abstract  = {Amidst the critical role that high-quality labeled data plays in advancing machine learning, the persistence of noise within widely-used datasets remains a challenge. While noise learning has gained traction within machine learning, particularly in computer vision, its exploration in text and multimodal classification domains has lagged. Furthermore, a comprehensive comparison of noise learning techniques in text and multimodal classification has been lacking, partly due to variations in experimental noise settings across prior studies. Addressing these gaps, this research introduces a pioneering Multimodal Infusion Joint Training (MinJoT) framework featuring a novel co-regularized loss function that seamlessly integrates multimodal information during joint training. This framework notably excels in maintaining model robustness amidst noisy data environments. Adapting established noise learning methods from computer vision to text classification, the study conducts extensive experiments across five English and Chinese textual and multimodal datasets, involving four methods, five noise modes, and seven noise rates. Critically, this work challenges the implicit assumption that widely-used datasets are devoid of noise, revealing that these datasets indeed encompass noise levels ranging from 0.61% to 15.77% which is defined as intrinsic noise in this study. For the first time, the study investigates the impact of intrinsic noise on model performance, categorizing it into distinct levels of ambiguity. To facilitate accurate method comparison, a new dataset, Golden-Chnsenticorp (G-Chnsenticorp), is introduced, carefully crafted to be free of intrinsic noise. This research establishes the inaugural noise learning benchmark for text classification, while simultaneously pioneering the first noise learning framework tailored for multimodal sentiment classification. Through these contributions, the study advances the understanding of noise learning in text and multimodal contexts, providing a novel framework, benchmarks, and insights to propel the field forward.},
  doi       = {10.1016/j.inffus.2023.102071},
  keywords  = {Multimodal,Noise learning,Target-oriented sentiment classification (TMSC),Text classification},
  publisher = {Elsevier BV},
}

@Article{tengfeil2024,
  author    = {Tengfei Liu and Yongli Hu and Junbin Gao and Jiapu Wang and Yanfeng Sun and Baocai Yin},
  journal   = {Neural Networks},
  title     = {Multi-modal long document classification based on Hierarchical Prompt and Multi-modal Transformer},
  year      = {2024},
  issn      = {0893-6080},
  month     = aug,
  pages     = {106322},
  volume    = {176},
  abstract  = {In the realm of long document classification (LDC), previous research has predominantly focused on modeling unimodal texts, overlooking the potential of multi-modal documents incorporating images. To address this gap, we introduce an innovative approach for multi-modal long document classification based on the Hierarchical Prompt and Multi-modal Transformer (HPMT). The proposed HPMT method facilitates multi-modal interactions at both the section and sentence levels, enabling a comprehensive capture of hierarchical structural features and complex multi-modal associations of long documents. Specifically, a Multi-scale Multi-modal Transformer (MsMMT) is tailored to capture the multi-granularity correlations between sentences and images. This is achieved through the incorporation of multi-scale convolutional kernels on sentence features, enhancing the model's ability to discern intricate patterns. Furthermore, to facilitate cross-level information interaction and promote learning of specific features at different levels, we introduce a Hierarchical Prompt (HierPrompt) block. This block incorporates section-level prompts and sentence-level prompts, both derived from a global prompt via distinct projection networks. Extensive experiments are conducted on four challenging multi-modal long document datasets. The results conclusively demonstrate the superiority of our proposed method, showcasing its performance advantages over existing techniques.},
  doi       = {10.1016/j.neunet.2024.106322},
  keywords  = {Multi-modal long document classification,Multi-modal transformer,Multi-scale multi-modal transformer,Prompt learning},
  pmid      = {38653128},
  publisher = {Elsevier BV},
}

@Article{ronghaop2024,
  author    = {Ronghao Pan and José Antonio García-Díaz and Miguel Ángel Rodríguez-García and Rafel Valencia-García},
  journal   = {Computer Standards &amp; Interfaces},
  title     = {Spanish MEACorpus 2023: A multimodal speech–text corpus for emotion analysis in Spanish from natural environments},
  year      = {2024},
  issn      = {0920-5489},
  month     = aug,
  pages     = {103856},
  volume    = {90},
  abstract  = {In human–computer interaction, emotion recognition provides a deeper understanding of the user's emotions, enabling empathetic and effective responses based on the user's emotional state. While deep learning models have improved emotion recognition solutions, it is still an active area of research. One important limitation is that most emotion recognition systems use only text as input, ignoring features such as voice intonation. Another limitation is the limited number of datasets available for multimodal emotion recognition. In addition, most published datasets contain emotions that are simulated by professionals and produce limited results in real-world scenarios. In other languages, such as Spanish, hardly any datasets are available. Therefore, our contributions to emotion recognition are as follows. First, we compile and annotate a new corpus for multimodal emotion recognition in Spanish (Spanish MEACorpus 2023), which contains 13.16 h of speech divided into 5129 segments labeled by considering Ekman's six basic emotions. The dataset is extracted from YouTube videos in natural environments. Second, we explore several deep learning models for emotion recognition using text- and audio-based features. Third, we evaluate different multimodal techniques to build a multimodal recognition system that improves the results of unimodal models, achieving a Macro F1-score of 87.745%, using late fusion with concatenation strategy approach.},
  doi       = {10.1016/j.csi.2024.103856},
  keywords  = {Deep-learning,Multimodal emotion analysis,Natural language processing,Speech emotion analysis,Text classification,Transformers},
  publisher = {Elsevier BV},
}

@Article{dasigiv2001,
  author    = {Venu Dasigi and Reinhold C. Mann and Vladimir A. Protopopescu},
  journal   = {Pattern Recognition},
  title     = {Information fusion for text classification — an experimental comparison},
  year      = {2001},
  issn      = {0031-3203},
  month     = dec,
  number    = {12},
  pages     = {2413--2425},
  volume    = {34},
  abstract  = {This article reports on our experiments and results on the effectiveness of different features sets and information fusion from some combinations of them in classifying free text documents into a given number of categories. We use different feature sets and integrate neural network learning into the method. The feature sets are based on the "latent semantics" of a reference library - a collection of documents adequately representing the desired concepts. We found that a larger reference library is not necessarily better. Information fusion almost always gives better results than the individual constituent feature sets, with certain combinations doing better than the others. ©. 2001 Published by Elsevier Science Ltd on behalf of Pattern Recognition Society.},
  doi       = {10.1016/s0031-3203(00)00171-0},
  issue     = {12},
  keywords  = {Features,Information fusion,Latent semantic indexing,Neural networks,Reference library,Text classification},
  publisher = {Elsevier BV},
}

@InProceedings{matsubara2005,
  author    = {Edson Takashi Matsubara and Maria Carolina Monard and Gustavo E.A.P.A. Batista},
  title     = {Multi-view semi-supervised learning: An approach to obtain different views from text datasets},
  year      = {2005},
  pages     = {97-104},
  publisher = {IOS Press BV},
  volume    = {132},
  abstract  = {The supervised machine learning approach usually requires a large number of labelled examples to learn accurately. However, labelling can be a costly and time consuming process, especially when manually performed. In contrast, unlabelled examples are usually inexpensive and easy to obtain. This is the case for text classification tasks involving on-line data sources, such as web pages, email and scientific papers. Semi-supervised learning, a relatively new area in machine learning, represents a blend of supervised and unsupervised learning, and has the potential of reducing the need of expensive labelled data whenever only a small set of labelled examples is available. Multi-view semi-supervised learning requires a partitioned description of each example into at least two distinct views. In this work, we propose a simple approach for textual documents pre-processing in order to easily construct the two different views required by any multi-view learning algorithm. Experimental results related to text classification are described, suggesting that our proposal to construct the views performs well in practice. © 2005 The authors. All rights reserved.},
  isbn      = {1586035681},
  issn      = {1879-8314},
  journal   = {Frontiers in Artificial Intelligence and Applications},
}

@InProceedings{zhangb2008,
  author    = {Zhang, Bang-zuo and Zuo, Wan-li},
  booktitle = {2008 First International Conference on Intelligent Networks and Intelligent Systems},
  title     = {Co-EM Support Vector Machine Based Text Classification from Positive and Unlabeled Examples},
  year      = {2008},
  month     = nov,
  pages     = {745--748},
  publisher = {IEEE},
  abstract  = {This paper has brought about a novel method based on multi-view algorithms for learning from positive and unlabeled examples (LPU). First we, with an improved 1-DNF method, split the text feature into a positive feature set (PF) and a negative feature set (NF). And we project each text vector on the two feature sets in turn. Then we use the co-EM SVM algorithm, which was previously used for semi-supervised learning. Finally, we select the better classifier for the result. Comprehensive evaluation has been performed on the Reuers-21578 collection which shows that our method is efficient and effective. © 2008 IEEE.},
  doi       = {10.1109/icinis.2008.29},
  isbn      = {9780769533919},
  journal   = {Proceedings - The 1st International Conference on Intelligent Networks and Intelligent Systems, ICINIS 2008},
}

@InProceedings{suns2008,
  author    = {Shiliang Sun},
  booktitle = {2008 IEEE International Conference on Data Mining Workshops},
  title     = {Semantic Features for Multi-view Semi-supervised and Active Learning of Text Classification},
  year      = {2008},
  month     = dec,
  pages     = {731--735},
  publisher = {IEEE},
  abstract  = {For multi-view learning, existing methods usually exploit originally provided features for classifier training, which ignore the latent correlation between different views. In this paper, semantic features integrating information from multiple views are extracted for pattern representation. Canonical correlation analysis is used to learn the representation of semantic spaces where semantic features are projections of original features on the basis vectors of the spaces. We investigate the feasibility of semantic features on two learning paradigms: semi-supervised learning and active learning. Experiments on text classification with two state-ofthe-art multi-view learning algorithms co-training and cotesting indicate that this use of semantic features can lead to a significant improvement of performance. © 2008 IEEE.},
  doi       = {10.1109/icdmw.2008.13},
  isbn      = {9780769535036},
  journal   = {Proceedings - IEEE International Conference on Data Mining Workshops, ICDM Workshops 2008},
}

@Article{gup2009,
  author    = {Gu, Ping and Zhu, QingSheng and Zhang, Cheng},
  journal   = {Computers &amp; Mathematics with Applications},
  title     = {A multi-view approach to semi-supervised document classification with incremental Naive Bayes},
  year      = {2009},
  issn      = {0898-1221},
  month     = mar,
  number    = {6},
  pages     = {1030--1036},
  volume    = {57},
  abstract  = {Many semi-supervised learning algorithms only consider the distribution of word frequency, ignoring the semantic and syntactic information underlying the documents. In this paper, we present a new multi-view approach for semi-supervised document classification by incorporating both semantic and syntactic information. For this purpose, a co-training style algorithm, Co-features, is proposed. In the phase of active querying, we assign a weight to each sample document according to its uncertainty factor. Then the most informative samples are selected and labeled by other "teachers". In contrast to batch training mode, we developed an incremental Naive Bayes update method, which allows for more efficient training even with a large pool of unlabeled data. Experimental results show that our algorithm works successfully on the datasets Reuters-21578 and WebKB, and is superior to Co-testing in the learning efficiency. © 2008 Elsevier Ltd. All rights reserved.},
  doi       = {10.1016/j.camwa.2008.10.025},
  issue     = {6},
  keywords  = {Active learning,Co-training,Multi-view,Semantic,Semi-supervised learning},
  publisher = {Elsevier BV},
}

@InProceedings{zhangx2009,
  author    = {Zhang, Xue and Zhao, Dong-yan and Chen, Li-wei and Min, Wang-hua},
  booktitle = {2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery},
  title     = {Batch Mode Active Learning Based Multi-view Text Classification},
  year      = {2009},
  pages     = {472--476},
  publisher = {IEEE},
  volume    = {7},
  abstract  = {The goal of active learning is to select the most informative examples for manual labeling in order to reduce the effort involved in acquiring labeled examples, which is very important for large-scale text classification. However, most of the previous studies in active learning have focused on selecting a single unlabeled example at a time which could be inefficient since the model has to be retrained for every new labeled example. In this paper we propose a novel simple batch mode active learning(BMAL) method based on farthest-first traversal to select a number of informative examples for labeling simultaneously in each iteration. Furthermore, we combine the BMAL with a multi-view framework in order to improve its execution efficiency. The k nearest neighbor(kNN) model is used as the baseline classifier for its simplicity and efficiency. Extensive experiments on standard dataset have shown that our algorithm is more effective than the single mode counterpart and the baseline classifier. © 2009 IEEE.},
  doi       = {10.1109/fskd.2009.495},
  isbn      = {9780769537351},
  journal   = {6th International Conference on Fuzzy Systems and Knowledge Discovery, FSKD 2009},
  keywords  = {Batch mode active learning,Multi-view learning,Text classification,k nearest neighbor(kNN)},
}

@InProceedings{chenb2009,
  author    = {Bin Chen and Bin Li and Zhisong Pan and Aimin Feng},
  booktitle = {2009 International Conference on Industrial and Information Systems},
  title     = {Document Classification with One-class Multiview Learning},
  year      = {2009},
  month     = apr,
  pages     = {289--292},
  publisher = {IEEE},
  abstract  = {Recently, automatic document classification has attracted a lot of attentions due to the large quantity of web documents. Amongst, a special case is to distinguish whether a document belongs to a target class (Directory) when only the documents of target class are given, which is a standard oneclass classification problem. Moreover, differed from other data, web pages have intrinsic (text) and extrinsic( hyperlink) features. Thus they are very suitable for multiview learning. To tackle the task of one-class document classification, a multiview one-class classifier is proposed, it utilizes the One-cluster clustering based data description (OCCDD) as the base one-class classifier, then gets a oneclass classifier in each view by setting a membership threshold, simultaneously, achieves the consensus of different views by a regularization term. Hereafter, different views boost each other, rather than ensemble the results independently or perform document recognition in single view case. We conduct the experiments on the standard WebKB dataset with OCCDD and the proposed multiview method. Experimental results show the good performance of the multiview method in terms of effectiveness and stability to parameter. © 2009 IEEE.},
  doi       = {10.1109/iis.2009.15},
  isbn      = {9780769536187},
  journal   = {Proceedings - 2009 International Conference on Industrial and Information Systems, IIS 2009},
}

@InProceedings{aminim2009,
  author    = {Massih-Reza Amini and Nicolas Usunier and Cyril Goutte},
  title     = {Learning from Multiple Partially Observed Views - an Application to Multilingual Text Categorization},
  year      = {2009},
  pages     = {28-36},
  publisher = {Neural Information Processing Systems},
  abstract  = {We address the problem of learning classifiers when observations have multiple views, some of which may not be observed for all examples. We assume the existence of view generating functions which may complete the missing views in an approximate way. This situation corresponds for example to learning text classifiers from multilingual collections where documents are not available in all languages. In that case, Machine Translation (MT) systems may be used to translate each document in the missing languages. We derive a generalization error bound for classifiers learned on examples with multiple artificially created views. Our result uncovers a trade-off between the size of the training set, the number of views, and the quality of the view generating functions. As a consequence, we identify situations where it is more interesting to use multiple views for learning instead of classical single view learning. An extension of this framework is a natural way to leverage unlabeled multi-view data in semi-supervised learning. Experimental results on a subset of the Reuters RCV1/RCV2 collections support our findings by showing that additional views obtained from MT may significantly improve the classification performance in the cases identified by our trade-off.},
  isbn      = {9781615679119},
  journal   = {Advances in Neural Information Processing Systems 22 - Proceedings of the 2009 Conference},
  url       = {https://www.semanticscholar.org/paper/82402bf63b039073e2027246d1d4332781b15002},
  venue     = {Neural Information Processing Systems},
}

@InProceedings{chens2009,
  author    = {Scott Deeann Chen and Vishal Monga and Pierre Moulin},
  booktitle = {2009 IEEE International Workshop on Multimedia Signal Processing},
  title     = {Meta-classifiers for multimodal document classification},
  year      = {2009},
  month     = oct,
  pages     = {1--6},
  publisher = {IEEE},
  abstract  = {This paper proposes learning algorithms for the problem of multimodal document classification. Specifically, we develop classifiers that automatically assign documents to categories by exploiting features from both text as well as image content. In particular, we use meta-classifiers that combine state-of-the-art text and image based classifiers into making joint decisions. The two meta classifiers we choose are based on support vector machines and Adaboost. Experiments on real-world databases from Wikipedia demonstrate the benefits of a joint exploitation of these modalities. © 2009 IEEE.},
  doi       = {10.1109/mmsp.2009.5293343},
  isbn      = {9781424444649},
  journal   = {2009 IEEE International Workshop on Multimedia Signal Processing, MMSP '09},
}

@Article{aminim2010,
  author    = {Amini, Massih-Reza and Goutte, Cyril},
  journal   = {Machine Learning},
  title     = {A co-classification approach to learning from multilingual corpora},
  year      = {2009},
  issn      = {1573-0565},
  month     = oct,
  number    = {1–2},
  pages     = {105--121},
  volume    = {79},
  abstract  = {We address the problem of learning text categorization from a corpus of multilingual documents. We propose a multiview learning, co-regularization approach, in which we consider each language as a separate source, and minimize a joint loss that combines monolingual classification losses in each language while ensuring consistency of the categorization across languages. We derive training algorithms for logistic regression and boosting, and show that the resulting categorizers outperform models trained independently on each language, and even, most of the times, models trained on the joint bilingual data. Experiments are carried out on a multilingual extension of the RCV2 corpus, which is available for benchmarking. © The Author(s) 2009.},
  doi       = {10.1007/s10994-009-5151-5},
  issue     = {1-2},
  keywords  = {Boosting,Logistic regression,Multilingual data,Text categorization},
  publisher = {Springer Science and Business Media LLC},
}

@InProceedings{zhangx2010b,
  author    = {Zhang, Xiao-Dan},
  booktitle = {2010 2nd International Conference on Education Technology and Computer},
  title     = {A general decision layer text classification fusion model},
  year      = {2010},
  month     = jun,
  pages     = {V5-239-V5-241},
  publisher = {IEEE},
  volume    = {5},
  abstract  = {An general decision layer text classification fusion model for higher precision, is proposed, which based on model theory of information fusion, and different classification algorithm of the feature layer fusion centre having different pre-processing, their classification results input into the decision layer fusion centre separately. And the final classification result output from decision layer fusion centre. KNN, SVM and BP Net are used in feature layer, and D-S Theory is used in decision layer. The model is realized in the experiment. From the experiment and contrast, the text classification fusion model can improve the classification precision effectively. © 2010 IEEE.},
  doi       = {10.1109/icetc.2010.5529774},
  isbn      = {9781424463688},
  journal   = {ICETC 2010 - 2010 2nd International Conference on Education Technology and Computer},
  keywords  = {Classification algorithm,Decision layer classification fusion model,Information fusion,Text classification},
}

@Article{suns2010,
  author    = {Shiliang Sun and David R. Hardoon},
  journal   = {Neurocomputing},
  title     = {Active learning with extremely sparse labeled examples},
  year      = {2010},
  issn      = {0925-2312},
  month     = oct,
  number    = {16–18},
  pages     = {2980--2988},
  volume    = {73},
  abstract  = {In the setting of active learning there exists a general assumption that labeled examples are available for training a classifier, which in turn is used to examine unlabeled data to select the most 'informative' examples for manual labeling. However, in some domain applications there are a limited number of labeled examples available, such as in the most extreme cases of having a single labeled example per category. In these scenarios, the most existing active learning methodologies cannot be directly applied without initially making an assumption on label assignment. In this paper we present a method for finding high-informative examples for manual labeling based on extremely limited labeled data available during training. We propose using canonical correlation analysis to investigate the correlation between different views of the available data and demonstrate that this measure can be used as a selection criterion for the novel application of active learning using only a single labeled example from each class. We demonstrate our method with promising experimental results on text classification, advertisement removal and multi-class image classification tasks. © 2010 Elsevier B.V.},
  doi       = {10.1016/j.neucom.2010.07.007},
  issue     = {16-18},
  keywords  = {Active learning,Canonical correlation analysis,Image classification,Multi-view learning,Text classification},
  publisher = {Elsevier BV},
}

@InProceedings{aminim2010b,
  author     = {Amini, Massih R. and Goutte, Cyril and Usunier, Nicolas},
  booktitle  = {Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval},
  title      = {Combining coregularization and consensus-based self-training for multilingual text categorization},
  year       = {2010},
  month      = jul,
  pages      = {475--482},
  publisher  = {ACM},
  series     = {SIGIR ’10},
  abstract   = {We investigate the problem of learning document classifiers in a multilingual setting, from collections where labels are only partially available. We address this problem in the framework of multiview learning, where different languages correspond to different views of the same document, combined with semi-supervised learning in order to benefit from unlabeled documents. We rely on two techniques, coregularization and consensus-based self-training, that combine multiview and semi-supervised learning in different ways. Our approach trains different monolingual classifiers on each of the views, such that the classifiers' decisions over a set of unlabeled examples are in agreement as much as possible, and iteratively labels new examples from another unlabeled training set based on a consensus across language-specific classifiers. We derive a boosting-based training algorithm for this task, and analyze the impact of the number of views on the semi-supervised learning results on a multilingual extension of the Reuters RCV1/RCV2 corpus using five different languages. Our experiments show that coregularization and consensus-based self-training are complementary and that their combination is especially effective in the interesting and very common situation where there are few views (languages) and few labeled documents available. © 2010 Crown in Right of Canada.},
  collection = {SIGIR ’10},
  doi        = {10.1145/1835449.1835529},
  isbn       = {9781605588964},
  journal    = {SIGIR 2010 Proceedings - 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  keywords   = {Learning from multiple views,Multilingual document classification,Semi-supervised learning},
}

@InProceedings{perezgraciat2010,
  author     = {Tomás Pérez-García and Carlos Pérez-Sancho and José M. Iñesta},
  booktitle  = {Proceedings of 3rd international workshop on Machine learning and music},
  title      = {Harmonic and instrumental information fusion for musical genre classification},
  year       = {2010},
  month      = oct,
  pages      = {49--52},
  publisher  = {ACM},
  series     = {MM ’10},
  abstract   = {This paper presents a musical genre classification system based on the combination of two kinds of information of very different nature: the instrumentation information contained in a MIDI file (metadata) and the chords that provide the harmonic structure of the musical score stored in that file (content). The fusion of these two information sources gives a single feature vector that represents the file and to which classification techniques usually utilized for text categorization tasks are applied. The classification task is performed under a probabilistic approach that has improved the results previously obtained for the same data using the instrumental or the chord information independently.},
  collection = {MM ’10},
  doi        = {10.1145/1878003.1878020},
  isbn       = {9781450301619},
  journal    = {MML'10 - Proceedings of the 3rd ACM International Workshop on Machine Learning and Music, Co-located with ACM Multimedia 2010},
  keywords   = {Genre classification,Multimodality},
}

@InProceedings{zhangx2010,
  author    = {Xiao-Dan Zhang},
  booktitle = {2010 International Conference on Web Information Systems and Mining},
  title     = {Study on Multi-layer Fusion Classification Model of Multi-media Information},
  year      = {2010},
  month     = oct,
  pages     = {216--218},
  publisher = {IEEE},
  volume    = {1},
  abstract  = {for higher text classification precision, a general fusion classification model and algorithm are proposed, which based on model theory of information fusion, adopting multi-Media information on the network. The model includes two layers, one is feature layer, which deals with different Media information with different classification algorithm, and inputs the classification results into the higher layer fusion centre separately. The other is fusion layer, which deals with the results from the feature layer, and concludes the final classification result. The experiment expresses the fusion model can improve the text classification precision effectively. © 2010 IEEE.},
  doi       = {10.1109/wism.2010.126},
  isbn      = {9780769542249},
  journal   = {2010 International Conference on Web Information Systems and Mining},
  keywords  = {information fusion,multi-information classifacation fusion model,text classifacationt},
}

@InBook{zhengw2011,
  author    = {Wenbin Zheng and Yuntao Qian and Hong Tang},
  pages     = {505--512},
  publisher = {Springer Berlin Heidelberg},
  title     = {Dimensionality Reduction with Category Information Fusion and Non-negative Matrix Factorization for Text Categorization},
  year      = {2011},
  isbn      = {9783642238963},
  volume    = {7004 LNAI},
  abstract  = {Dimensionality reduction can efficiently improve computing performance of classifiers in text categorization, and non-negative matrix factorization could map the high dimensional term space into a low dimensional semantic subspace easily. Meanwhile, the non-negative of the basis vectors could provide a meaningful explanation for the semantic subspace. However, it usually could not achieve a satisfied classification performance because it is sensitive to the noise, data missing and outlier as a linear reconstruction method. This paper proposes a novel approach in which the train text and its category information are fused and a transformation matrix that maps the term space into a semantic subspace is obtained by a basis orthogonality non-negative matrix factorization and truncation. Finally, the dimensionality can be reduced aggressively with these transformations. Experimental results show that the proposed approach remains a good classification performance in a very low dimensional case. © 2011 Springer-Verlag.},
  booktitle = {Artificial Intelligence and Computational Intelligence},
  doi       = {10.1007/978-3-642-23896-3_62},
  issn      = {1611-3349},
  issue     = {PART 3},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords  = {Category Fusion,Dimensionality reduction,Non-negative Matrix Factorization,Text Categorization},
}

@InProceedings{guyo2012,
  author        = {Yuhong Guo and Min Xiao},
  title         = {Cross Language Text Classification via Subspace Co-regularized Multi-view Learning},
  year          = {2012},
  pages         = {1615-1622},
  volume        = {2},
  abstract      = {In many multilingual text classification problems, the documents in different languages often share the same set of categories. To reduce the labeling cost of training a classification model for each individual language, it is important to transfer the label knowledge gained from one language to another language by conducting cross language classification. In this paper we develop a novel subspace co-regularized multi-view learning method for cross language text classification. This method is built on parallel corpora produced by machine translation. It jointly minimizes the training error of each classifier in each language while penalizing the distance between the subspace representations of parallel documents. Our empirical study on a large set of cross language text classification tasks shows the proposed method consistently outperforms a number of inductive methods, domain adaptation methods, and multi-view learning methods.},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1206.6481},
  eprint        = {1206.6481},
  isbn          = {9781450312851},
  journal       = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012},
  url           = {https://www.semanticscholar.org/paper/f8b345f1203f153087fe23b56afad286dccb8da4},
  venue         = {International Conference on Machine Learning},
}

@InProceedings{kovesim2012,
  author     = {Kovesi, Michelle and Goutte, Cyril and Amini, Massih-Reza},
  booktitle  = {Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval},
  title      = {Fast on-line learning for multilingual categorization},
  year       = {2012},
  month      = aug,
  pages      = {1071--1072},
  publisher  = {ACM},
  series     = {SIGIR ’12},
  abstract   = {Multiview learning has been shown to be a natural and efficient framework for supervised or semi-supervised learning of multilingual document categorizers. The state-of-the-art co-regularization approach relies on alternate minimizations of a combination of language-specific categorization errors and a disagreement between the outputs of the monolingual text categorizers. This is typically solved by repeatedly training categorizers on each language with the appropriate regularizer. We extend and improve this approach by introducing an on-line learning scheme, where language-specific updates are interleaved in order to iteratively optimize the global cost in one pass. Our experimental results show that this produces similar performance as the batch approach, at a fraction of the computational cost. © 2012 Authors.},
  collection = {SIGIR ’12},
  doi        = {10.1145/2348283.2348474},
  isbn       = {9781450316583},
  journal    = {SIGIR'12 - Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval},
  keywords   = {multilingual text categorisation,on-line learning},
}

@InProceedings{yangp2012,
  author   = {Pei Yang and Wei Gao and Qi Tan and Kam-Fai Wong},
  title    = {Information-theoretic Multi-view Domain Adaptation},
  year     = {2012},
  pages    = {270-274},
  volume   = {2},
  abstract = {We use multiple views for cross-domain document classification. The main idea is to strengthen the views' consistency for target data with source training data by identifying the correlations of domain-specific features from different domains. We present an Information-theoretic Multi-view Adaptation Model (IMAM) based on a multi-way clustering scheme, where word and link clusters can draw together seemingly unrelated domain-specific features from both sides and iteratively boost the consistency between document clusterings based on word and link views. Experiments show that IMAM significantly outperforms state-of-the-art baselines.},
  isbn     = {9781937284251},
  journal  = {50th Annual Meeting of the Association for Computational Linguistics, ACL 2012 - Proceedings of the Conference},
  url      = {https://www.semanticscholar.org/paper/47b13e59e543ce671b3e0161bf89b6aa5750add3},
  venue    = {Annual Meeting of the Association for Computational Linguistics},
}

@Article{lig2012,
  author    = {Guangxia Li and Kuiyu Chang and Steven C.H. Hoi},
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  title     = {Multiview Semi-Supervised Learning with Consensus},
  year      = {2012},
  issn      = {1041-4347},
  month     = nov,
  number    = {11},
  pages     = {2040--2051},
  volume    = {24},
  abstract  = {Obtaining high-quality and up-to-date labeled data can be difficult in many real-world machine learning applications. Semi-supervised learning aims to improve the performance of a classifier trained with limited number of labeled data by utilizing the unlabeled ones. This paper demonstrates a way to improve the transductive SVM, which is an existing semi-supervised learning algorithm, by employing a multiview learning paradigm. Multiview learning is based on the fact that for some problems, there may exist multiple perspectives, so called views, of each data sample. For example, in text classification, the typical view contains a large number of raw content features such as term frequency, while a second view may contain a small but highly informative number of domain specific features. We propose a novel two-view transductive SVM that takes advantage of both the abundant amount of unlabeled data and their multiple representations to improve classification result. The idea is straightforward: train a classifier on each of the two views of both labeled and unlabeled data, and impose a global constraint requiring each classifier to assign the same class label to each labeled and unlabeled sample. We also incorporate manifold regularization, a kind of graph-based semi-supervised learning method into our framework. The proposed two-view transductive SVM was evaluated on both synthetic and real-life data sets. Experimental results show that our algorithm performs up to 10 percent better than a single-view learning approach, especially when the amount of labeled data is small. The other advantage of our two-view semi-supervised learning approach is its significantly improved stability, which is especially useful when dealing with noisy data in real-world applications. © 2012 IEEE.},
  doi       = {10.1109/tkde.2011.160},
  issue     = {11},
  keywords  = {Artificial intelligence,learning systems,multiview learning,semi-supervised learning,support vector machines},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@InProceedings{zhangb2013,
  author    = {Zhang, Bo and Shi, Zhong-Zhi},
  booktitle = {2013 IEEE International Conference on Big Data},
  title     = {Classification of big velocity data via cross-domain Canonical Correlation Analysis},
  year      = {2013},
  month     = oct,
  pages     = {493--498},
  publisher = {IEEE},
  abstract  = {Many classification techniques work well only under a common assumption that the training and test data are drawn from the same feature space and the same distribution. However, big velocity data usually show disobedience of this assumption. For example, in the field of web-document classification, new document is continuously emerging every day. Transfer learning aims at leveraging the knowledge in labeled source domains to predict the unlabeled data in a target domain, where the distributions are different in domains. As one of the important research directions of transfer learning, one kind of approaches focus on the correspondence between pivot features and all the other specific features from different domains, to extract some relevant features that may reduce the difference between the domains, have attracted wide attention and study. However, the limitation caused by the vague meanings in different domains prevents these algorithms from further improvement. To tackle this problem, we propose a cross-domain canonical correlation analysis algorithm called CD-CCA by applying Canonical Correlation Analysis (CCA) to transfer learning. CD-CCA can learn a semantic space of multi-view correspondences from different domains respectively and transfer the knowledge by dimensionality reduction in a multi-view way. Experimental results on the 144×6 classification problems in 20Newsgroups, show that CD-CCA can significantly improve the prediction accuracy. © 2013 IEEE.},
  doi       = {10.1109/bigdata.2013.6691612},
  isbn      = {9781479912926},
  journal   = {Proceedings - 2013 IEEE International Conference on Big Data, Big Data 2013},
  keywords  = {big velocity data classification,canonical correlation analysis,transfer learning},
}

@Article{liy2013,
  author    = {LI, YANJUN and HSU, D. FRANK and CHUNG, SOON M.},
  journal   = {International Journal on Artificial Intelligence Tools},
  title     = {Combination of multiple feature selection methods for text categorization by using combinatorial fusion analysis and rank-score characteristic},
  year      = {2013},
  issn      = {1793-6349},
  month     = apr,
  number    = {02},
  pages     = {1350001},
  volume    = {22},
  abstract  = {Effective feature selection methods are important for improving the efficiency and accuracy of text categorization algorithms by removing redundant and irrelevant terms from the corpus. Extensive research has been done to improve the performance of individual feature selection methods. However, it is always a challenge to come up with an individual feature selection method which would outperform other methods in most cases. In this paper, we explore the possibility of improving the overall performance by combining multiple individual feature selection methods. In particular, we propose a method of combining multiple feature selection methods by using an information fusion paradigm, called Combinatorial Fusion Analysis (CFA). A rank-score function and its associated graph, called rank-score graph, are adopted to measure the diversity of different feature selection methods. Our experimental results demonstrated that a combination of multiple feature selection methods can outperform a single method only if each individual feature selection method has unique scoring behavior and relatively high performance. Moreover, it is shown that the rank-score function and rank-score graph are useful for the selection of a combination of feature selection methods. © World Scientific Publishing Company.},
  doi       = {10.1142/s0218213013500012},
  issue     = {2},
  keywords  = {Feature selection,combinatorial fusion analysis (CFA),rank combination,rank-score function,score combination,text categorization},
  publisher = {World Scientific Pub Co Pte Lt},
}

@InProceedings{perinaa2013,
  author    = {A. Perina and N. Jojic and M. Bicego and Andrzej Truski},
  title     = {Documents as multiple overlapping windows into grids of counts},
  year      = {2013},
  publisher = {Neural information processing systems foundation},
  abstract  = {In text analysis documents are often represented as disorganized bags of words; models of such count features are typically based on mixing a small number of topics [1, 2]. Recently, it has been observed that for many text corpora documents evolve into one another in a smooth way, with some features dropping and new ones being introduced. The counting grid [3] models this spatial metaphor literally: it is a grid of word distributions learned in such a way that a document's own distribution of features can be modeled as the sum of the histograms found in a window into the grid. The major drawback of this method is that it is essentially a mixture and all the content must be generated by a single contiguous area on the grid. This may be problematic especially for lower dimensional grids. In this paper, we overcome this issue by introducing the Componential Counting Grid which brings the componential nature of topic models to the basic counting grid. We evaluated our approach on document classification and multimodal retrieval obtaining state of the art results on standard benchmarks.},
  issn      = {1049-5258},
  journal   = {Advances in Neural Information Processing Systems},
  url       = {https://www.semanticscholar.org/paper/30db7dff9104ac512c1d15c0f48dd4c1b930abcb},
  venue     = {Neural Information Processing Systems},
}

@InBook{longg2013,
  author    = {Guodong Long and Jing Jiang},
  pages     = {456--467},
  publisher = {Springer Berlin Heidelberg},
  title     = {Graph Based Feature Augmentation for Short and Sparse Text Classification},
  year      = {2013},
  isbn      = {9783642539145},
  volume    = {8346 LNAI},
  abstract  = {Short text classification, such as snippets, search queries, micro-blogs and product reviews, is a challenging task mainly because short texts have insufficient co-occurrence information between words and have a very spare document-term representation. To address this problem, we propose a novel multi-view classification method by combining both the original document-term representation and a new graph based feature representation. Our proposed method uses all documents to construct a neighbour graph by using the shared co-occurrence words. Multi-Dimensional Scaling (MDS) is further applied to extract a low-dimensional feature representation from the graph, which is augmented with the original text features for learning. Experiments on several benchmark datasets show that the proposed multi-view classifier, trained from augmented feature representation, obtains significant performance gain compared to the baseline methods. © Springer-Verlag 2013.},
  booktitle = {Advanced Data Mining and Applications},
  doi       = {10.1007/978-3-642-53914-5_39},
  issn      = {1611-3349},
  issue     = {PART 1},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords  = {Graph based method,Multi-Dimensional scaling,Multi-view learning,Short text,Text classification},
}

@InProceedings{zhangd2013,
  author     = {Zhang, Dan and He, Jingrui and Lawrence, Richard},
  booktitle  = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
  title      = {MI2LS: multi-instance learning from multiple informationsources},
  year       = {2013},
  month      = aug,
  pages      = {149--157},
  publisher  = {Association for Computing Machinery (ACM)},
  series     = {KDD’ 13},
  volume     = {Part F128815},
  abstract   = {In Multiple Instance Learning (MIL), each entity is normally expressed as a set of instances. Most of the current MIL methods only deal with the case when each instance is represented by one type of features. However, in many real world applications, entities are often described from several different information sources/views. For example, when applying MIL to image categorization, the characteristics of each image can be derived from both its RGB features and SIFT features. Previous research work has shown that, in traditional learning methods, leveraging the consistencies between different information sources could improve the classification performance drastically. Out of a similar motivation, to incorporate the consistencies between different information sources into MIL, we propose a novel research framework - Multi-Instance Learning from Multiple Information Sources (MI2LS). Based on this framework, an algorithm - Fast MI2LS (FMI2LS) is designed, which combines Constraint Concave-Convex Programming (CCCP) method and an adapted Stoachastic Gradient Descent (SGD) method. Some theoretical analysis on the optimality of the adapted SGD method and the generalized error bound of the formulation are given based on the proposed method. Experimental results on document classification and a novel application - Insider Threat Detection (ITD), clearly demonstrate the superior performance of the proposed method over state-of-The-Art MIL methods.},
  collection = {KDD’ 13},
  doi        = {10.1145/2487575.2487651},
  isbn       = {9781450321747},
  journal    = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  keywords   = {Multi-instance learning,Multi-view learning,Stoachastic gradient descent},
}

@InBook{cristanim2014,
  author    = {Matteo Cristani and Claudio Tomazzoli},
  pages     = {490--499},
  publisher = {Springer International Publishing},
  title     = {A Multimodal Approach to Exploit Similarity in Documents},
  year      = {2014},
  isbn      = {9783319074559},
  volume    = {8481},
  abstract  = {Automated document classification process extracts information with a systematic analysis of the content of documents. This is an active research field of growing importance due to the large amount of electronic documents produced in the world wide web and available thanks to diffused technologies including mobile ones. Several application areas benefit from automated document classification, including document archiving, invoice processing in business environments, press releases and research engines. Current tools classify or "tag" either text or images separately.In this paper we show how, by linking image and text-based contents together, a technology improves fundamental document management tasks like retrieving information from a database or automated documents. We present an investigation of a model of conceptual spaces for investigation using joint information sources from the text and the images forming complex documents. We present a formal model and the computable algorithms and the dataset from which we took a subset to make experiments and relative tests and results.},
  booktitle = {Modern Advances in Applied Intelligence},
  doi       = {10.1007/978-3-319-07455-9_51},
  issn      = {1611-3349},
  journal   = {Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science)},
}

@Article{liuj2014,
  author    = {Jin Liu and Juan Li and Xiaoping Sun and Yuan Xie and Jeff Lei and Qiping Hu},
  journal   = {Future Generation Computer Systems},
  title     = {An Embedded Co-AdaBoost based construction of software document relation coupled resource spaces for cyber–physical society},
  year      = {2014},
  issn      = {0167-739X},
  month     = mar,
  pages     = {198--210},
  volume    = {32},
  abstract  = {Software is a very important means of achieving the vision of the cyber-physical society. Software document relation coupled Resource Spaces prompts the cyber-physical society by facilitating the reuse of software design knowledge. The establishment of software document relation coupled Resource Spaces faces the scarcity of labeled data that helps discovering software document relations between resources dwelling in different Resource Spaces. This paper proposes the Embedded Co-AdaBoost algorithm to overcome this challenge by making the best use of easily available unlabeled data, integrating multi-view learning into the AdaBoost and leveraging the advantages of Co-training for performance enhancement. Compared with conventional AdaBoost, the experiment illustrates the effectiveness of the Embedded Co-AdaBoost in the convergence rate, the accuracy and the steady performance. The empirical experience demonstrates the ability of the Embedded Co-AdaBoost in prompting the development of software document relation coupled Resource Spaces.©2013 Published by Elsevier Ltd. All rights reserved.},
  doi       = {10.1016/j.future.2012.12.017},
  issue     = {1},
  keywords  = {Coupled resource spaces,Embedded Co-AdaBoost,Software document classification,Software document relation},
  publisher = {Elsevier BV},
}

@Article{yangp2014,
  author    = {Yang, P. and Gao, W.},
  journal   = {Journal of Artificial Intelligence Research},
  title     = {Information-Theoretic Multi-view Domain Adaptation: A Theoretical and Empirical Study},
  year      = {2014},
  issn      = {1076-9757},
  month     = mar,
  pages     = {501--525},
  volume    = {49},
  abstract  = {Multi-view learning aims to improve classification performance by leveraging the consistency among different views of data. The incorporation of multiple views was paid little attention in the studies of domain adaptation, where the view consistency based on source data is largely violated in the target domain due to the distribution gap between different domain data. In this paper, we leverage multiple views for cross-domain document classification. The central idea is to strengthen the views' consistency on target data by identifying the associations of domain-specific features from different domains. We present an Information-theoretic Multi-view Adaptation Model (IMAM) using a multi-way clustering scheme, where word and link clusters can draw together seemingly unrelated features across domains, which boosts the consistency between document clusterings that are based on the respective word and link views. Moreover, we demonstrate that IMAM can always find the document clustering with the minimal disagreement rate to the overlap of viewbased clusterings. We provide both theoretical and empirical justifications of the proposed method. Our experiments show that IMAM significantly outperforms traditional multi-view algorithm cotraining, the co-training-based adaptation algorithm CODA, the single-view transfer model CoCC and the large-margin-based multi-view transfer model MVTL-LM. © 2014 AI Access Foundation. All rights reserved.},
  doi       = {10.1613/jair.4190},
  publisher = {AI Access Foundation},
}

@InBook{liparas2014,
  author    = {Dimitris Liparas and Yaakov HaCohen-Kerner and Anastasia Moumtzidou and Stefanos Vrochidis and Ioannis Kompatsiaris},
  pages     = {63--75},
  publisher = {Springer International Publishing},
  title     = {News Articles Classification Using Random Forests and Weighted Multimodal Features},
  year      = {2014},
  isbn      = {9783319129792},
  volume    = {8849},
  abstract  = {This research investigates the problem of news articles classification. The classification is performed using N-gram textual features extracted from text and visual features generated from one representative image. The application domain is news articles written in English that belong to four categories: Business-Finance, Lifestyle-Leisure, Science-Technology and Sports downloaded from three well-known news web-sites (BBC, Reuters, and TheGuardian). Various classification experiments have been performed with the Random Forests machine learning method using N-gram textual features and visual features from a representative image. Using the N-gram textual features alone led to much better accuracy results (84.4%) than using the visual features alone (53%). However, the use of both N-gram textual features and visual features led to slightly better accuracy results (86.2%). The main contribution of this work is the introduction of a news article classification framework based on Random Forests and multimodal features (textual and visual), as well as the late fusion strategy that makes use of Random Forests operational capabilities.},
  booktitle = {Multidisciplinary Information Retrieval},
  doi       = {10.1007/978-3-319-12979-2_6},
  issn      = {1611-3349},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords  = {Document classification,Fusion,Multimodal,N-gram features,News articles,Random Forests,Supervised learning,Visual features},
}

@InProceedings{liaox2015,
  author    = {Xinpeng L. Liao and Chengcui Zhang and Ariel D. Smith and Grant T. Savage},
  booktitle = {2015 IEEE International Conference on Information Reuse and Integration},
  title     = {A Multi-topic Meta-classification Scheme for Analyzing Lobbying Disclosure Data},
  year      = {2015},
  month     = aug,
  pages     = {349--356},
  publisher = {IEEE},
  abstract  = {For the functioning of American democracy, the Lobbying Disclosure Act (LDA), for the very first time, provides data to empirically research interest groups behaviors and their influence on congressional policymaking. One of the main research challenges is to automatically find the topic(s), by short &amp; sparse text classification, in a large corpus of unorganized, semi-structured, and poorly connected lobbying filings to reveal the underlying purpose(s) of these lobbying activities. Common techniques for alleviating data sparseness are to enrich the context of data by external information. This paper, however, proposed an inter-disciplinary yet practical solution to this problem using a Multi-Topic Meta-Classification (MTMC) scheme built upon a set of semantic attributes (i.e., General Issue, Specific Issue, and Bill Info.), integrated with a domain-specific Policy Agenda (PA) coding/labeling procedure. First, multi-label base-classifiers that have been transformed into multi-class classification problems were learned from the abovementioned three semantic sources, respectively, second, to render reliability classification, one meta-classifier per attribute was trained based on meta-instances dataset labeled in a cross-validation fashion, third, the final prediction is made via fusing the reliable outputs of such ensembles of classifiers. Experiments demonstrated satisfactory classification performance with various evaluation measures on such a real-world textual dataset that poses many challenges including problems with noisy data and semantic ambiguity.},
  doi       = {10.1109/iri.2015.60},
  isbn      = {9781467366564},
  journal   = {Proceedings - 2015 IEEE 16th International Conference on Information Reuse and Integration, IRI 2015},
  keywords  = {information fusion,machine learning applications,meta-classifier,multi-class &amp; multi-label classification},
}

@InProceedings{brefeldu2015,
  author     = {Ulf Brefeld},
  booktitle  = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
  title      = {Multi-view learning with dependent views},
  year       = {2015},
  month      = apr,
  pages      = {865--870},
  publisher  = {Association for Computing Machinery (ACM)},
  series     = {SAC 2015},
  volume     = {13-17-April-2015},
  abstract   = {Multi-view algorithms, such as co-training and co-EM, utilize unlabeled data when the available attributes can be split into independent and compatible subsets. Experiments have shown that multi-view learning is sometimes beneficial for problems for which the independence assumption is not satis fied. In practice, unfortunately, it is not possible to measure the dependency between two attribute sets; hence, there is no criterion which allows to decide whether multi-view learning is applicable. We conduct experiments with various text classification problems and investigate on the effectiveness of the co-trained SVM and the co-EM SVM under various conditions, including violations of the independence assumption. We identify the error correlation coefficient of the initial classifiers as an elaborate indicator of the expected benefit of multi-view learning. Copyright is held by the owner/author(s).},
  collection = {SAC 2015},
  doi        = {10.1145/2695664.2695829},
  isbn       = {9781450331968},
  journal    = {Proceedings of the ACM Symposium on Applied Computing},
  keywords   = {Multi-view learning},
}

@Article{fakri2015,
  author    = {Fakeri-Tabrizi, Ali and Amini, Massih-Reza and Goutte, Cyril and Usunier, Nicolas},
  journal   = {Neurocomputing},
  title     = {Multiview self-learning},
  year      = {2015},
  issn      = {0925-2312},
  month     = may,
  pages     = {117--127},
  volume    = {155},
  abstract  = {In many applications, observations are available with different views. This is, for example, the case with image-text classification, multilingual document classification or document classification on the web. In addition, unlabeled multiview examples can be easily acquired, but assigning labels to these examples is usually a time consuming task. We describe a multiview self-learning strategy which trains different voting classifiers on different views. The margin distributions over the unlabeled training data, obtained with each view-specific classifier are then used to estimate an upper-bound on their transductive Bayes error. Minimizing this upper-bound provides an automatic margin-threshold which is used to assign pseudo-labels to unlabeled examples. Final class labels are then assigned to these examples, by taking a vote on the pool of the previous pseudo-labels. New view-specific classifiers are then trained using the labeled and pseudo-labeled training data. We consider applications to image-text classification and to multilingual document classification. We present experimental results on the NUS-WIDE collection and on Reuters RCV1-RCV2 which show that despite its simplicity, our approach is competitive with other state-of-the-art techniques.},
  doi       = {10.1016/j.neucom.2014.12.041},
  keywords  = {Image annotation,Multilingual document categorization,Multiview learning,Self-learning},
  publisher = {Elsevier BV},
}

@InBook{iglesias2016,
  author    = {Eva Lorenzo Iglesias and Adrían Seara Vieira and Lourdes Borrajo Diz},
  pages     = {66--78},
  publisher = {Springer International Publishing},
  title     = {An HMM-Based Multi-view Co-training Framework for Single-View Text Corpora},
  year      = {2016},
  isbn      = {9783319320342},
  volume    = {9648},
  abstract  = {Multi-view algorithms such as co-training improve the accuracy of text classification because they optimize the functions to exploit different views of the same input data. However, despite being more promising than the single-view approaches, document datasets often have no natural multiple views available. This study proposes an HMM-based algorithm to generate a new view from a standard text dataset, and a co-training framework where this view generation is applied. Given a dataset and a user classifier model as input, the goal of our framework is to improve the classifier performance by increasing the labelled document pool, taking advantage of the multi-view semi-supervised co-training algorithm. The novel architecture was tested using two different standard text corpora: Reuters and 20 Newsgroups and a classical SVM classifier. The results obtained are promising, showing a significant increase in the efficiency of the classifier compared to a single-view approach.},
  booktitle = {Hybrid Artificial Intelligent Systems},
  doi       = {10.1007/978-3-319-32034-2_6},
  issn      = {1611-3349},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords  = {Co-training,Hidden Markov Model,Multi-view,Text classification},
}

@InProceedings{rajendran2016,
  author    = {Janarthanan Rajendran and Mitesh M. Khapra and Sarath Chandar and Balaraman Ravindran},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  title     = {Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning},
  year      = {2016},
  pages     = {171--181},
  publisher = {Association for Computing Machinery (ACM)},
  abstract  = {Recently there has been a lot of interest in learning common representations for multiple views of data. Typically, such common representations are learned using a parallel corpus between the two views (say, 1M images and their English captions). In this work, we address a real-world scenario where no direct parallel data is available between two views of interest (say, V1 and V2) but parallel data is available between each of these views and a pivot view (V3). We propose a model for learning a common representation for V1, V2 and V3 using only the parallel data available between V1V3 and V2V3. The proposed model is generic and even works when there are n views of interest and only one pivot view which acts as a bridge between them. There are two specific downstream applications that we focus on (i) transfer learning between languages L1,L2,...,Ln using a pivot language L and (ii) cross modal access between images and a language L1 using a pivot language L2. Our model achieves state-of-the-art performance in multilingual document classification on the publicly available multilingual TED corpus and promising results in multilingual multimodal retrieval on a new dataset created and released as a part of this work.},
  doi       = {10.18653/v1/n16-1021},
  isbn      = {9781941643914},
  journal   = {2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference},
}

@Article{xux2016,
  author    = {Xinxing Xu and Wen Li and Dong Xu and Ivor W. Tsang},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title     = {Co-Labeling for Multi-View Weakly Labeled Learning},
  year      = {2016},
  issn      = {2160-9292},
  month     = jun,
  number    = {6},
  pages     = {1113--1125},
  volume    = {38},
  abstract  = {It is often expensive and time consuming to collect labeled training samples in many real-world applications. To reduce human effort on annotating training samples, many machine learning techniques (e.g., semi-supervised learning (SSL), multi-instance learning (MIL), etc.) have been studied to exploit weakly labeled training samples. Meanwhile, when the training data is represented with multiple types of features, many multi-view learning methods have shown that classifiers trained on different views can help each other to better utilize the unlabeled training samples for the SSL task. In this paper, we study a new learning problem called multi-view weakly labeled learning, in which we aim to develop a unified approach to learn robust classifiers by effectively utilizing different types of weakly labeled multi-view data from a broad range of tasks including SSL, MIL and relative outlier detection (ROD). We propose an effective approach called co-labeling to solve the multi-view weakly labeled learning problem. Specifically, we model the learning problem on each view as a weakly labeled learning problem, which aims to learn an optimal classifier from a set of pseudo-label vectors generated by using the classifiers trained from other views. Unlike traditional co-training approaches using a single pseudo-label vector for training each classifier, our co-labeling approach explores different strategies to utilize the predictions from different views, biases and iterations for generating the pseudo-label vectors, making our approach more robust for real-world applications. Moreover, to further improve the weakly labeled learning on each view, we also exploit the inherent group structure in the pseudo-label vectors generated from different strategies, which leads to a new multi-layer multiple kernel learning problem. Promising results for text-based image retrieval on the NUS-WIDE dataset as well as news classification and text categorization on several real-world multi-view datasets clearly demonstrate that our proposed co-labeling approach achieves state-of-the-art performance for various multi-view weakly labeled learning problems including multi-view SSL, multi-view MIL and multi-view ROD.},
  doi       = {10.1109/tpami.2015.2476813},
  issue     = {6},
  keywords  = {multi-instance learning,multi-view learning,relative outlier detection,semi-supervised learning,weakly labeled learning},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@InProceedings{sinorar2016,
  author    = {Roberta A. Sinoara and Rafael G. Rossi and Solange O. Rezende},
  booktitle = {2016 23rd International Conference on Pattern Recognition (ICPR)},
  title     = {Semantic role-based representations in text classification},
  year      = {2016},
  month     = dec,
  pages     = {2313--2318},
  publisher = {IEEE},
  volume    = {0},
  abstract  = {Although good results for automatic text classification can be achieved with the use of bag-of-words representation, this model is not suitable for all classification problems and richer text representations can be required. In this paper, we proposed two text representation models based on semantic role labels and analyzed them in text classification scenarios. We also evaluated the combination of bag-of-words with a semantic representation considering ensemble multi-view strategies. We explored different classification problems for two text collections and pointed out situations that require more than a bag-of-words. The experimental evaluation indicates that the combination of bag-of-words and a text representation based on semantic role labels can improve text classification accuracies.},
  doi       = {10.1109/icpr.2016.7899981},
  isbn      = {9781509048472},
  issn      = {1051-4651},
  journal   = {Proceedings - International Conference on Pattern Recognition},
}

@InProceedings{xuh2016,
  author     = {Xu, Haotian and Dong, Ming and Zhu, Dongxiao and Kotov, Alexander and Carcone, April Idalski and Naar-King, Sylvie},
  booktitle  = {Proceedings of the 7th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
  title      = {Text Classification with Topic-based Word Embedding and Convolutional Neural Networks},
  year       = {2016},
  month      = oct,
  pages      = {88--97},
  publisher  = {Association for Computing Machinery (ACM)},
  series     = {BCB ’16},
  abstract   = {Recently, distributed word embeddings trained by neural language models are commonly used for text classification with Convolutional Neural Networks (CNNs). In this paper, we propose a novel neural language model, Topic-based Skip-gram, to learn topic-based word embeddings for biomedical literature indexing with CNNs. Topic-based Skip-gram leverages textual content with topic models, e.g., Latent Dirichlet Allocation (LDA), to capture precise topic-based word relationship and then integrate it into distributed word embedding learning. We then describe two multimodal CNN architectures, which are able to employ different kinds of word embeddings at the same time for text classification. Through extensive experiments conducted on several realworld datasets, we demonstrate that combination of our Topic-based Skip-gram and multimodal CNN architectures outperforms state-of-the-art methods in biomedical literature indexing, clinical note annotation and general textual benchmark dataset classification.},
  collection = {BCB ’16},
  doi        = {10.1145/2975167.2975176},
  isbn       = {9781450342254},
  journal    = {ACM-BCB 2016 - 7th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
  keywords   = {Convolutional Neural Networks,Medical subject headings,Text classification,Word embeddings},
}

@Article{huz2017,
  author    = {Ze Hu and Zhan Zhang and Haiqin Yang and Qing Chen and Decheng Zuo},
  journal   = {Journal of Biomedical Informatics},
  title     = {A deep learning approach for predicting the quality of online health expert question-answering services},
  year      = {2017},
  issn      = {1532-0464},
  month     = jul,
  pages     = {241--253},
  volume    = {71},
  abstract  = {Recently, online health expert question-answering (HQA) services (systems) have attracted more and more health consumers to ask health-related questions everywhere at any time due to the convenience and effectiveness. However, the quality of answers in existing HQA systems varies in different situations. It is significant to provide effective tools to automatically determine the quality of the answers. Two main characteristics in HQA systems raise the difficulties of classification: (1) physicians’ answers in an HQA system are usually written in short text, which yields the data sparsity issue; (2) HQA systems apply the quality control mechanism, which refrains the wisdom of crowd. The important information, such as the best answer and the number of users’ votes, is missing. To tackle these issues, we prepare the first HQA research data set labeled by three medical experts in 90 days and formulate the problem of predicting the quality of answers in the system as a classification task. We not only incorporate the standard textual feature of answers, but also introduce a set of unique non-textual features, i.e., the popular used surface linguistic features and the novel social features, from other modalities. A multimodal deep belief network (DBN)-based learning framework is then proposed to learn the high-level hidden semantic representations of answers from both textual features and non-textual features while the learned joint representation is fed into popular classifiers to determine the quality of answers. Finally, we conduct extensive experiments to demonstrate the effectiveness of including the non-textual features and the proposed multimodal deep learning framework.},
  doi       = {10.1016/j.jbi.2017.06.012},
  keywords  = {Deep belief network,Deep learning,Multimodal learning,Online health expert question-answering services,Social features,Surface linguistic features},
  pmid      = {28606870},
  publisher = {Elsevier BV},
}

@InBook{akhtiamovo2017,
  author    = {Oleg Akhtiamov and Dmitrii Ubskii and Evgeniia Feldina and Aleksei Pugachev and Alexey Karpov and Wolfgang Minker},
  pages     = {152--161},
  publisher = {Springer International Publishing},
  title     = {Are You Addressing Me? Multimodal Addressee Detection in Human-Human-Computer Conversations},
  year      = {2017},
  isbn      = {9783319664293},
  volume    = {10458 LNAI},
  abstract  = {The goal of addressee detection is to answer the question ‘Are you addressing me?’ In order to participate in multiparty conversations, a spoken dialogue system is supposed to determine whether a user is addressing the system or another human. The present paper describes three levels of speech and text analysis (acoustical, lexical, and syntactical) for multimodal addressee detection and reveals the connection between them and the classification performance for different categories of speech. We propose several classification models and compare their performance with the results of the original research performed by the authors of the Smart Video Corpus which we use in our computations. Our most effective meta-classifier working with acoustical, syntactical, and lexical features provides an unweighted average recall equal to 0.917, showing a nine percent advantage over the best baseline model, though the baseline classifier additionally uses head orientation data. We also propose an LSTM neural network for text classification which replaces the lexical and the syntactical classifier by a single model reaching the same performance as the most effective meta-classifier does, despite the fact that this meta-model additionally analyses acoustical data.},
  booktitle = {Speech and Computer},
  doi       = {10.1007/978-3-319-66429-3_14},
  issn      = {1611-3349},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords  = {Data fusion,Long short-term memory,Multimodal interaction,Off-talk,Speaking style,Spoken dialogue system,Text classification},
}

@InBook{zhanz2017,
  author    = {Zengrong Zhan and Zhengming Ma},
  pages     = {41--51},
  publisher = {Springer International Publishing},
  title     = {Document Analysis Based on Multi-view Intact Space Learning with Manifold Regularization},
  year      = {2017},
  isbn      = {9783319677774},
  volume    = {10559 LNCS},
  abstract  = {Document analysis plays an important role in our life, and traditional models like Latent Semantic Analysis (LSI) or Latent Dirichlet Allocation (LDA) cannot handle data from many sources. Multi-view learning technology like Multi-view Intact Space Learning (MISL), which integrates the complementary information on multiple views to discover a latent intact representation of the data, is effective for image or video application. But the model has not been applied to multi-lingual documents and has not considered the intrinsic geometrical and discriminating structure of the document data. To overcome this issue, we assume that if documents are close in the origin representation, they should also be close in the intact space representation. And we introduce a manifold regularization term to MISL so that the data is more smoothly in latent space. We conduct classification experiments on 10505 Wiki documents we crawled, and the result shows that it is outperforming TFIDF, LSI, LDA, and MISL.},
  booktitle = {Intelligence Science and Big Data Engineering},
  doi       = {10.1007/978-3-319-67777-4_4},
  issn      = {1611-3349},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords  = {Document classification,Manifold regularization,Multi-view learning},
}

@Article{schmittm2017,
  author        = {Maximilian Schmitt and Björn Schuller},
  journal       = {Journal of Machine Learning Research},
  title         = {openXBOW - Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit},
  year          = {2017},
  issn          = {1533-7928},
  month         = {10},
  pages         = {1-5},
  volume        = {18},
  abstract      = {We introduce openXBOW, an open-source toolkit for the generation of bag-of-words (BoW) representations from multimodal input. In the BoW principle, word histograms were first used as features in document classification, but the idea was and can easily be adapted to, e.g., acoustic or visual low-level descriptors, introducing a prior step of vector quantisation. The openXBOW toolkit supports arbitrary numeric input features and text input and concatenates computed subbags to a final bag. It provides a variety of extensions and options. To our knowledge, openXBOW is the first publicly available toolkit for the generation of crossmodal bags-of-words. The capabilities of the tool are exemplified in two sample scenarios: time-continuous speech-based emotion recognition and sentiment analysis in tweets where improved results over other feature representation forms were observed.},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1605.06778},
  eprint        = {1605.06778},
  keywords      = {Bag-of-words,Feature learning,Histogram feature representations,Multimodal signal processing},
  publisher     = {Microtome Publishing},
  url           = {https://www.semanticscholar.org/paper/16cfa5ab8025bccca5c6bd07b62f7716d6926ade},
  venue         = {Journal of machine learning research},
}

@InBook{xuc2017,
  author    = {Cheng Xu and Yue Wu and Zongtian Liu},
  pages     = {124--134},
  publisher = {Springer International Publishing},
  title     = {Multimodal Fusion with Global and Local Features for Text Classification},
  year      = {2017},
  isbn      = {9783319700878},
  volume    = {10634 LNCS},
  abstract  = {Text classification is a crucial task in natural language processing. Due to the characteristics of text structure, achieving the best result remains an ongoing challenge. In this paper, we propose an ensemble model which outperforms the state-of-the-art. We first utilize rule-based n-gram approach to extend corpus. Then two different features, global dependencies of word and local semantic feature, are extracted by gated recurrent unit and global average pooling model respectively. In order to take advantage of the complementarity of the global and local features, a decision-level fusion is applied to fuse those different kinds of features. We evaluate the quality of our model on various public datasets, including sentiment analysis, ontology classification and text categorization. Experimental results show that our model can effectively learn representations for language modeling, and achieves the best accuracy of text categorization.},
  booktitle = {Neural Information Processing},
  doi       = {10.1007/978-3-319-70087-8_14},
  issn      = {1611-3349},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords  = {Global average pooling,Global feature,Semantic feature,Text classification},
}

@Article{pengj2018,
  author    = {Jing Peng and Alex J. Aved and Guna Seetharaman and Kannappan Palaniappan},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  title     = {Multiview Boosting With Information Propagation for Classification},
  year      = {2018},
  issn      = {2162-2388},
  month     = mar,
  number    = {3},
  pages     = {657--669},
  volume    = {29},
  abstract  = {Multiview learning has shown promising potential in many applications. However, most techniques are focused on either view consistency, or view diversity. In this paper, we introduce a novel multiview boosting algorithm, called Boost.SH, that computes weak classifiers independently of each view but uses a shared weight distribution to propagate information among the multiple views to ensure consistency. To encourage diversity, we introduce randomized Boost.SH and show its convergence to the greedy Boost.SH solution in the sense of minimizing regret using the framework of adversarial multiarmed bandits. We also introduce a variant of Boost.SH that combines decisions from multiple experts for recommending views for classification. We propose an expert strategy for multiview learning based on inverse variance, which explores both consistency and diversity. Experiments on biometric recognition, document categorization, multilingual text, and yeast genomic multiview data sets demonstrate the advantage of Boost.SH (85%) compared with other boosting algorithms like AdaBoost (82%) using concatenated views and substantially better than a multiview kernel learning algorithm (74%).},
  doi       = {10.1109/tnnls.2016.2637881},
  issue     = {3},
  keywords  = {Biometrics,boosting,classification,convergence,data fusion,multiarmed bandits,multiview learning},
  pmid      = {28060713},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{argon2018,
  author    = {Mario Ezra Aragón and Adrian Pastor Lopez-Monroy},
  journal   = {CEUR Workshop Proceedings},
  title     = {A Straightforward Multimodal Approach for Author Profiling: Notebook for PAN at CLEF 2018},
  year      = {2018},
  issn      = {1613-0073},
  volume    = {2125},
  abstract  = {In this paper we evaluate different strategies from the literature for text and image classification at PAN 2018. The main objective of this shared task is the identification of the gender of different users by using tweets and images posted. We evaluate four popular strategies for the text representation: 1) Bag of Terms (BoT), 2) Second Order Attributes (SOA) representation, 3) Convolutional Neural Network (CNN) models and 4) an Ensemble of n-grams at word and character level. For the image representation we used a Convolutional Neural Network (CNN) based on [6]. We observed that the n-grams Ensemble presented the highest performance. For our participation we chose the Ensemble and perform an early fusion with the image representation to create a multimodal representation.},
  keywords  = {Author Profiling,Bag of Words,CNN,Text Classification,Text Mining},
  publisher = {CEUR-WS},
  url       = {https://www.semanticscholar.org/paper/f728dea872696fb16e7f1f81e0c9e1c414f7e5f2},
  venue     = {Conference and Labs of the Evaluation Forum},
}

@InProceedings{ferreira2018,
  author    = {Ferreira, Charles Henrique Porto and De Franca, Fabricio Olivetti and Medeiros, Debora R.},
  booktitle = {2018 IEEE Congress on Evolutionary Computation (CEC)},
  title     = {Combining Multiple Views from a Distance Based Feature Extraction for Text Classification},
  year      = {2018},
  month     = jul,
  pages     = {1--8},
  publisher = {IEEE},
  abstract  = {Text Mining is a challenging task due to the lack of a naturally structured representation and the high dimensionality induced by the feature extraction techniques commonly used. Different feature extractions can lead to multiple views that can capture different aspects of the text documents being analyzed. The combination of these features can lead to a better accuracy in classification tasks but, also, an undesirable increase in the number of features. In this work, we investigate the use of a feature extraction technique called DCDistance used as a multiple feature extraction for text documents combined with a Genetic Algorithm based feature selection, hereby called MVDCD. The results show that the main advantage of MVDCD is that the dimensionality is reduced by more than 90% while significantly increasing the classification accuracy when compared to vanilla DCDistance and other feature selections techniques. A side effect of the use of DCDistance and MVDCD is the possibility of model interpretability, as the extracted features are explicit.},
  doi       = {10.1109/cec.2018.8477772},
  isbn      = {9781509060177},
  journal   = {2018 IEEE Congress on Evolutionary Computation, CEC 2018 - Proceedings},
  keywords  = {feature extraction,feature selection,genetic algorithm,multiview,text mining},
}

@InProceedings{guptad2018,
  author    = {Divam Gupta and Indira Sen and Niharika Sachdeva and Ponnurangam Kumaraguru and Arun Balaji Buduru},
  booktitle = {2018 IEEE International Conference on Cognitive Computing (ICCC)},
  title     = {Empowering First Responders through Automated Multimodal Content Moderation},
  year      = {2018},
  month     = jul,
  pages     = {1--8},
  publisher = {IEEE},
  abstract  = {Social media enables users to spread information and opinions, including in times of crisis events such as riots, protests or uprisings. Sensitive event-related content can lead to repercussions in the real world. Therefore it is crucial for first responders, such as law enforcement agencies, to have ready access, and the ability to monitor the propagation of such content. Obstacles to easy access include a lack of automatic moderation tools targeted for first responders. Efforts are further complicated by the multimodal nature of content which may have either textual and pictorial aspects. In this work, as a means of providing intelligence to first responders, we investigate automatic moderation of sensitive event-related content across the two modalities by exploiting recent advances in Deep Neural Networks (DNN). We use a combination of image classification with Convolutional Neural Networks (CNN) and text classification with Recurrent Neural Networks (RNN). Our multilevel content classifier is obtained by fusing the image classifier and the text classifier. We utilize feature engineering for preprocessing but bypass it during classification due to our use of DNNs while achieving coverage by leveraging community guidelines. Our approach maintains a low false positive rate and high precision by learning from a weakly labeled dataset and then, by learning from an expert annotated dataset. We evaluate our system both quantitatively and qualitatively to gain a deeper understanding of its functioning. Finally, we benchmark our technique with current approaches to combating sensitive content and find that our system outperforms by 16% in accuracy.},
  doi       = {10.1109/iccc.2018.00008},
  isbn      = {9781538672419},
  journal   = {Proceedings - 2018 IEEE International Conference on Cognitive Computing, ICCC 2018 - Part of the 2018 IEEE World Congress on Services},
  keywords  = {Image analysis,Multimodal detection,Natural language processing},
}

@InBook{akhiamov2018,
  author    = {Oleg Akhtiamov and Vasily Palkov},
  pages     = {1--10},
  publisher = {Springer International Publishing},
  title     = {Gaze, Prosody and Semantics: Relevance of Various Multimodal Signals to Addressee Detection in Human-Human-Computer Conversations},
  year      = {2018},
  isbn      = {9783319995793},
  volume    = {11096 LNAI},
  abstract  = {The present research is focused on multimodal addressee detection in human-human-computer conversations. A modern spoken dialogue system operating under realistic conditions that may include multiparty interaction (several people solve a cooperative task by addressing the system while talking to each other) is supposed to distinguish machine- from human-addressed utterances. Machine-addressed queries should be directly responded to, while human-addressed utterances should be either ignored or processed in an implicit way. We propose a multimodal system performing the visual, acoustic-prosodic, and textual analysis of users’ utterances. We managed to outperform the existing baseline for the Smart Video Corpus by applying our system. We also investigated the performance of different models for separate speech categories with various speech spontaneity and determined that the acoustical model has difficulties in classifying constrained speech, and the textual model performs worse for spontaneous speech, while the performance of the visual model drops for read human-addressed speech and for spontaneous human-addressed speech significantly due to the ambiguous behaviour of users.},
  booktitle = {Speech and Computer},
  doi       = {10.1007/978-3-319-99579-3_1},
  issn      = {1611-3349},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords  = {Computational Paralinguistics,Frontal face detection,Off-Talk,Speaking style,Spoken dialogue system,Text classification},
}

@InProceedings{tellez2018,
  author    = {Eric S. Tellez and Sabino Miranda-Jiménez and Daniela Moctezuma and Mario Graff and Vladimir Salgado and José Ortiz-Bejar},
  title     = {Gender identification through multi-modal Tweet analysis using MicroTC and Bag of Visual Words: Notebook for PAN at CLEF 2018},
  year      = {2018},
  publisher = {CEUR-WS},
  volume    = {2125},
  abstract  = {This manuscript describes our solution to solve the Author Profiling task at PAN'18. In this edition, the task asks for identifying the user's gender using both their Tweets containing texts and images. We used our MicroTC (μTC) text classification framework to cope with the text problem, and a novel approach to Bag of Visual Words to solve the image classification, designed for this task to solve the image classification. Finally, we tried to improve the final prediction using a combination of both approaches.},
  issn      = {1613-0073},
  journal   = {CEUR Workshop Proceedings},
}

@InProceedings{matricm2018,
  author    = {Matej Martinc and Blaž Škrlj and Senja Pollak},
  title     = {Multilingual gender classification with multi-view deep learning notebook for PAN at CLEF 2018},
  year      = {2018},
  publisher = {CEUR-WS},
  volume    = {2125},
  abstract  = {We present the results of a gender identification performed on the data set of tweets and images prepared for the PAN 2018 Author profiling shared task. In this work we propose a hybrid neural network architecture for gender classification, capable of leveraging heterogeneous textual and image information sources. The proposed approach is based on state-of-the-art deep architectures for natural language processing, combined with a pretrained image classification architecture via a custom output combination scheme. Text classification model combines character level, word level and document level information in order to produce stable and accurate predictions on three different languages, achieving the highest accuracy of 79% on the English test set. Image classification architecture relies on the hypothesis that the authors have a gender bias when it comes to publishing images of people and has a structure of a two-phased pipeline containing two models, one for face detection and the other for face gender classification. Classifying author's gender from posted images proved to be harder than from text, with our image classification model achieving the best accuracy of only 58.26% on the English test set. The results on the official PAN test set also confirmed slight synergy effects between the two models when combined. The proposed approach was 8th in the global ranking of PAN 2018 Author profiling shared task. For future work, we plan to focus on improving the results of the image classification model by testing different architectures for image object detection and image captioning, which might prove helpful in extracting topical information from images that could be used for gender prediction.},
  issn      = {1613-0073},
  journal   = {CEUR Workshop Proceedings},
}

@Article{zhup2018,
  author    = {Pengfei Zhu and Qi Hu and Qinghua Hu and Changqing Zhang and Zhizhao Feng},
  journal   = {Pattern Recognition},
  title     = {Multi-view label embedding},
  year      = {2018},
  issn      = {0031-3203},
  month     = dec,
  pages     = {126--135},
  volume    = {84},
  abstract  = {Multi-label classification has been successfully applied to image annotation, information retrieval, text categorization, etc. When the number of classes increases significantly, the traditional multi-label learning models will become computationally impractical. Label space dimension reduction (LSDR) is then developed to alleviate the effect of the high dimensionality of labels. However, almost all the existing LSDR methods focus on single-view learning. In this paper, we develop a multi-view label embedding (MVLE) model by exploiting the multi-view correlations. The label space and feature space of each view are bridged by a latent space. To exploit the consensus among different views, multi-view latent spaces are correlated by Hilbert–Schmidt independence criterion(HSIC). For a test sample, it is firstly embedded to the latent space of each view and then projected to the label space. The prediction is conducted by combining the multi-view outputs. Experiments on benchmark databases show that MVLE outperforms the state-of-the-art LSDR algorithms in both multi-view settings and different multi-view learning strategies.},
  doi       = {10.1016/j.patcog.2018.07.009},
  keywords  = {Label space dimension reduction,Multi-label classification,Multi-view label embedding},
  publisher = {Elsevier BV},
}

@Article{mmironczuk2020,
  author    = {Marcin Michał Mirończuk and Jarosław Protasiewicz},
  journal   = {Applied Intelligence},
  title     = {Recognising innovative companies by using a diversified stacked generalisation method for website classification},
  year      = {2019},
  issn      = {1573-7497},
  month     = jun,
  number    = {1},
  pages     = {42--60},
  volume    = {50},
  abstract  = {In this paper, we propose a classification system which is able to decide whether a company is innovative or not, based only on its public website available on the internet. As innovativeness plays a crucial role in the development of myriad branches of the modern economy, an increasing number of entities are expending effort to be innovative. Thus, a new issue has appeared: how can we recognise them? Not only is grasping the idea of innovativeness challenging for humans, but also impossible for any known machine learning algorithm. Therefore, we propose a new indirect technique: a diversified stacked generalisation method, which is based on a combination of a multi-view approach and a genetic algorithm. The proposed approach achieves better performance than all other classification methods which include: (i) models trained on single datasets; or (ii) a simple voting method on these models. Furthermore, in this study, we check if unaligned feature space improves classification results. The proposed solution has been extensively evaluated on real data collected from companies’ websites. The experimental results verify that the proposed method improves the classification quality of websites which might represent innovative companies.},
  doi       = {10.1007/s10489-019-01509-1},
  issue     = {1},
  keywords  = {Ensemble learning,Innovative detection,Multi-view learning,Multiple views,Text classification},
  publisher = {Springer Science and Business Media LLC},
}

@InProceedings{anget2018,
  author    = {Tato Ange and Nkambou Roger and Dufresne Aude and Frasson Claude},
  booktitle = {2018 International Joint Conference on Neural Networks (IJCNN)},
  title     = {Semi-Supervised Multimodal Deep Learning Model for Polarity Detection in Arguments},
  year      = {2018},
  month     = jul,
  pages     = {1--8},
  publisher = {IEEE},
  volume    = {2018-July},
  abstract  = {Deep learning has been successfully applied to many tasks such as image classification, feature learning, Text classification (sentiments analysis or opinion mining) etc. However, little research has focused on extracting polarity of sentiments expressed in text using a multimodal architecture. In other words, no researches take in consideration the multimodal nature of human behaviors before classifying sentiments. The representation of a person (also call User Modeling in some domains such as Intelligent Tutoring Systems) is an important feature to take in consideration if one wants to extract subjective information such as the polarity of sentiments expressed by the person. To design an effective representation of a user, it is important to consider all sources of data informing about its current state. We present a usersensitive deep multimodal architecture which takes advantage of deep learning and user data to extract a rich latent representation of a user. This rich latent representation mainly helps in text classification tasks. The architecture consists of the combination of a Long Short-Term Memory (LSTM), LSTM-Auto-Encoder, Convolutional Neural Networks and multiple Deep Neural Networks, in order to support the multimodality of data. The resulting model has been tested on a public multimodal dataset and is able to achieve best results compared to state-of-the-art algorithms for a similar task: Detection of opinion polarity. The results suggest that the latent representation learnt from multimodal data helps in the discrimination of polarity of opinion.},
  doi       = {10.1109/ijcnn.2018.8489342},
  isbn      = {9781509060146},
  journal   = {Proceedings of the International Joint Conference on Neural Networks},
  keywords  = {Convolutional Neural Network,Long Short-Term Memory,Long Short-Term Memory Autoencoder,Multimodal Deep Learning,Opinion mining,Semi-Supervised Learning,Text Classification,User modeling},
}

@InBook{akhtiamov2019,
  author    = {Oleg Akhtiamov and Dmitrii Fedotov and Wolfgang Minker},
  pages     = {20--30},
  publisher = {Springer International Publishing},
  title     = {A Comparative Study of Classical and Deep Classifiers for Textual Addressee Detection in Human-Human-Machine Conversations},
  year      = {2019},
  isbn      = {9783030260613},
  volume    = {11658 LNAI},
  abstract  = {The problem of addressee detection (AD) arises in multiparty conversations involving several dialogue agents. In order to maintain such conversations in a realistic manner, an automatic spoken dialogue system is supposed to distinguish between computer- and human-directed utterances since the latter utterances either need to be processed in a specific way or should be completely ignored by the system. In the present paper, we consider AD to be a text classification problem and model three aspects of users’ speech (syntactical, lexical, and semantical) that are relevant to AD in German. We compare simple classifiers operating with supervised text representations learned from in-domain data and more advanced neural network-based models operating with unsupervised text representations learned from in- and out-of-domain data. The latter models provide a small yet significant AD performance improvement over the classical ones on the Smart Video Corpus. A neural network-based semantical model determines the context of the first four words of an utterance to be the most informative for AD, significantly surpasses syntactical and lexical text classifiers and keeps up with a baseline multimodal metaclassifier that utilises acoustical information in addition to textual data. We also propose an effective approach to building representations for out-of-vocabulary words.},
  booktitle = {Speech and Computer},
  doi       = {10.1007/978-3-030-26061-3_3},
  issn      = {1611-3349},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords  = {Human-computer interaction,Speaking style,Spoken dialogue system,Text classification},
}

@InProceedings{hoylea2019,
  author        = {Hoyle, Alexander Miserlis and Wolf-Sonkin, Lawrence and Wallach, Hanna and Cotterell, Ryan and Augenstein, Isabelle},
  booktitle     = {Proceedings of the 2019 Conference of the North},
  title         = {Combining Sentiment Lexica with a Multi-View Variational Autoencoder},
  year          = {2019},
  pages         = {635--640},
  publisher     = {Association for Computational Linguistics (ACL)},
  volume        = {1},
  abstract      = {When assigning quantitative labels to a dataset, different methodologies may rely on different scales. In particular, when assigning polarities to words in a sentiment lexicon, annotators may use binary, categorical, or continuous labels. Naturally, it is of interest to unify these labels from disparate scales to both achieve maximal coverage over words and to create a single, more robust sentiment lexicon while retaining scale coherence. We introduce a generative model of sentiment lexica to combine disparate scales into a common latent representation. We realize this model with a novel multi-view variational autoencoder (VAE), called SentiVAE. We evaluate our approach via a downstream text classification task involving nine English-Language sentiment analysis datasets; our representation outperforms six individual sentiment lexica, as well as a straightforward combination thereof.},
  archiveprefix = {arXiv},
  doi           = {10.18653/v1/n19-1065},
  eprint        = {1904.02839},
  isbn          = {9781950737130},
  journal       = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
  url           = {https://www.semanticscholar.org/paper/fade3b6a731f14d83bff864e87bc689c44a0399c},
  venue         = {North American Chapter of the Association for Computational Linguistics},
}

@Article{wangh2019,
  author    = {Huibing Wang and Jinjia Peng and Xianping Fu},
  journal   = {Neurocomputing},
  title     = {Co-regularized multi-view sparse reconstruction embedding for dimension reduction},
  year      = {2019},
  issn      = {0925-2312},
  month     = jun,
  pages     = {191--199},
  volume    = {347},
  abstract  = {With the development of information technology, we have witnessed an age of data explosion which produces a large variety of data filled with redundant information. Because dimension reduction is an essential tool which embeds high-dimensional data into a lower-dimensional subspace to avoid redundant information, it has attracted interests from researchers all over the world. However, facing with features from multiple views, it's difficult for most dimension reduction methods to fully comprehended multi-view features and integrate compatible and complementary information from these features to construct low-dimensional subspace directly. Furthermore, most multi-view dimension reduction methods cannot handle features from nonlinear spaces with high dimensions. Therefore, how to construct a multi-view dimension reduction methods which can deal with multi-view features from high-dimensional nonlinear space is of vital importance but challenging. In order to address this problem, we proposed a novel method named Co-regularized Multi-view Sparse Reconstruction Embedding (CMSRE) in this paper. By exploiting correlations of sparse reconstruction from multiple views, CMSRE is able to learn local sparse structures of nonlinear manifolds from multiple views and constructs significative low-dimensional representations for them. Due to the proposed co-regularized scheme, correlations of sparse reconstructions from multiple views are preserved by CMSRE as much as possible. Furthermore, sparse representation produces more meaningful correlations between features from each single view, which helps CMSRE to gain better performances. Various evaluations based on the applications of document classification, face recognition and image retrieval can demonstrate the effectiveness of the proposed approach on multi-view dimension reduction.},
  doi       = {10.1016/j.neucom.2019.03.080},
  keywords  = {Dimension reduction,Multi-view,Multi-view sparse reconstruction Embedding,Sparse reconstruction},
  publisher = {Elsevier BV},
}

@InBook{chens2019,
  author    = {Si Chen and Liangguo Wang and Wan Li and Kun Zhang},
  pages     = {179--190},
  publisher = {Springer International Publishing},
  title     = {Deep Learning Method with Attention for Extreme Multi-label Text Classification},
  year      = {2019},
  isbn      = {9783030298944},
  volume    = {11672 LNAI},
  abstract  = {Extreme multi-label text classification (XMTC), the problem of finding the most relevant label subset of each document from hundreds or even millions labels, has been a practical and important problem since the boom of big data. Significant progress has been made in recent years by the development of machine learning methods. However, although deep learning method has beaten traditional method in other related areas, it has no clear advantage in XMTC when we consider the performance of prediction. In order to improve the performance of deep learning method for Extreme multi-label text classification, we propose a novel feature extraction method to better explore the text space. Specifically, we build the model consisting of attention mechanism, convolutional neural network and recurrent neural network to extract multi-view features. Extensive experiments on four public available datasets show that our method achieves better performance than several strong baselines, including traditional methods and deep learning methods.},
  booktitle = {PRICAI 2019: Trends in Artificial Intelligence},
  doi       = {10.1007/978-3-030-29894-4_14},
  issn      = {1611-3349},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords  = {Deep learning,Extreme multi-label classification,Feature extraction},
}

@Article{mmironczuk2019,
  author    = {Marcin Michał Mirończuk and Jarosław Protasiewicz and Witold Pedrycz},
  journal   = {Expert Systems with Applications},
  title     = {Empirical evaluation of feature projection algorithms for multi-view text classification},
  year      = {2019},
  issn      = {0957-4174},
  month     = sep,
  pages     = {97--112},
  volume    = {130},
  abstract  = {This study aims to propose (i) a multi-view text classification method and (ii) a ranking method that allows for selecting the best information fusion layer among many variations. Multi-view document classification is worth a detailed study as it makes it possible to combine different feature sets into yet another view that further improves text classification. For this purpose, we propose a multi-view framework for text classification that is composed of two levels of information fusion. At the first level, classifiers are constructed using different data views, i.e. different vector space models by various machine learning algorithms. At the second level, the information fusion layer uses input information using a features projection method and a meta-classifier modelled by a selected machine learning algorithm. A final decision based on classification results produced by the models positioned at the first layer is reached. Moreover, we propose a ranking method to assess various configurations of the fusion layer. We use heuristics that utilise statistical properties of F-score values calculated for classification results produced at the fusion layer. The information fusion layer of the classification framework and ranking method has been empirically evaluated. For this purpose, we introduce a use case checking whether companies’ domains identify their innovativeness. The results empirically demonstrate that the information fusion layer enhances classification quality. The Friedman's aligned rank and Wilcoxon signed-rank statistical tests and the effect size support this hypothesis. In addition, the Spearman statistical test carried out for the obtained results demonstrated that the assessment made by the proposed ranking method converges to a well-established method named Hellinger - The Technique for Order Preference by Similarity to Ideal Solution (H-TOPSIS). Thus, the proposed approach may be used for the assessment of classifier performance.},
  doi       = {10.1016/j.eswa.2019.04.020},
  keywords  = {Evaluation method,Information fusion,Information fusion in text classification,Multi-view document classification,Multi-view text classification,Ranking method},
  publisher = {Elsevier BV},
}

@InProceedings{ravikiranm2019,
  author     = {Manikandan Ravikiran and Krishna Madgula},
  booktitle  = {Proceedings of the ACM Workshop on Crossmodal Learning and Application},
  title      = {Fusing Deep Quick Response Code Representations Improves Malware Text Classification},
  year       = {2019},
  month      = jun,
  pages      = {11--18},
  publisher  = {Association for Computing Machinery (ACM)},
  series     = {ICMR ’19},
  abstract   = {Recently, multimodal processing for various text classification tasks such as emotion recognition, sentiment analysis, author profiling etc. have gained traction due to its potential to improve performance by leveraging complementary sources of information such as texts, images and speech. In this line, we focus on multimodal malware text classification. However unlike traditional tasks such as emotions recognition and sentiment analysis generating complementary domain information is difficult in cyber security, leading to little focus of such a multimodal idea in context of malware classification. As such, in this work we propose to address this gap by improving malware text classification task by leveraging Quick Response (QR) codes generated from the same as complementary information. With superior capacity of Convolutional Neural Network's (CNN) to process images, we fuse the representations from CNN's for both text and image data in multiple ways, where we show that using complementary information from QR codes improves the performance of the task of malware text classification thereby achieving new state-of-the-art and creating the very first multimodal benchmark on malware text classification.},
  collection = {ICMR ’19},
  doi        = {10.1145/3326459.3329166},
  isbn       = {9781450367806},
  journal    = {WCRML 2019 - Proceedings of the ACM Workshop on Crossmodal Learning and Application},
  keywords   = {Deep learning,Malware text classification,Multimodal fusion},
}

@InProceedings{jainr2019,
  author    = {Rajiv Jain and Curtis Wigington},
  booktitle = {2019 International Conference on Document Analysis and Recognition (ICDAR)},
  title     = {Multimodal Document Image Classification},
  year      = {2019},
  month     = sep,
  pages     = {71--77},
  publisher = {IEEE},
  abstract  = {State-of-the-art methods for document image classification rely on visual features extracted by deep convolutional neural networks (CNNs). These methods do not utilize rich semantic information present in the text of the document, which can be extracted using Optical Character Recognition (OCR). We first study the performance of state-of-the-art text classification approaches when applied to noisy text obtained from OCR. We then show that fusing this textual information with visual CNN methods produces state-of-the-art results on the RVL-CDIP classification dataset.},
  doi       = {10.1109/icdar.2019.00021},
  isbn      = {9781728128610},
  issn      = {1520-5363},
  journal   = {Proceedings of the International Conference on Document Analysis and Recognition, ICDAR},
  keywords  = {Classification,Document Image,Multimodal},
}

@Article{wangh2020,
  author    = {Huibing Wang and Lin Feng and Adong Kong and Bo Jin},
  journal   = {Soft Computing},
  title     = {Multi-view reconstructive preserving embedding for dimension reduction},
  year      = {2019},
  issn      = {1433-7479},
  month     = sep,
  number    = {10},
  pages     = {7769--7780},
  volume    = {24},
  abstract  = {With the development of feature extraction technique, one sample always can be represented by multiple features which are located in different high-dimensional spaces. Because multiple features can reflect one same sample from various perspectives, there must be compatible and complementary information among the multiple views. Therefore, it’s natural to learn information from multiple views to obtain better performance. However, most multi-view dimension reduction methods cannot handle multiple features from nonlinear space with high dimensions. To address this problem, we propose a novel multi-view dimension reduction method named multi-view reconstructive preserving embedding (MRPE) in this paper. MRPE reconstructs each sample by utilizing its k nearest neighbors. The similarities between each sample and its neighbors are mapped into lower-dimensional space in order to preserve the underlying neighborhood structure of the original manifold. MRPE fully exploits correlations between each sample and its neighbors from multiple views by linear reconstruction. Furthermore, MRPE constructs an optimization problem and derives an iterative procedure to obtain the low-dimensional embedding. Various evaluations based on the applications of document classification, face recognition and image retrieval demonstrate the effectiveness of our proposed approach on multi-view dimension reduction.},
  doi       = {10.1007/s00500-019-04395-4},
  issue     = {10},
  keywords  = {Dimension reduction,Linear reconstruction,Multi-view,Multi-view reconstructive preserving embedding},
  publisher = {Springer Science and Business Media LLC},
}

@Article{hey2019,
  author    = {Yiwei He and Yingjie Tian and Dalian Liu},
  journal   = {Neurocomputing},
  title     = {Multi-view transfer learning with privileged learning framework},
  year      = {2019},
  issn      = {0925-2312},
  month     = mar,
  pages     = {131--142},
  volume    = {335},
  abstract  = {In this paper, we present a multi-view transfer learning model named Multi-view Transfer Discriminative Model (MTDM) for both image and text classification tasks. Transfer learning, which aims to learn a robust classifier for the target domain using data from a different distribution, has been proved to be effective in many real-world applications. However, most of the existing transfer learning methods map across domain data into a high-dimension space which the distance between domains is closed. This strategy always fails in the multi-view scenario. On the contrary, the multi-view learning methods are also difficult to extend in the transfer learning settings. One of our goals in this paper is to develop a model which can perform better in both multi-view and transfer learning settings. On the one hand, the problem of multi-view is implemented by the paradigm of learning using privileged information (LUPI), which could guarantee the principle of complementary and consensus. On the other hand, the model adequately utilizes the source domain data to build a robust classifier for the target domain. We evaluate our model on both image and text classification tasks and show the effectiveness compared with other baseline approaches.},
  doi       = {10.1016/j.neucom.2019.01.019},
  keywords  = {Learning using privileged information,Multi-view learning,Support vector machine,Transfer learning},
  publisher = {Elsevier BV},
}

@Article{bhatt2019,
  author    = {Gaurav Bhatt and Piyush Jha and Balasubramanian Raman},
  journal   = {Pattern Recognition},
  title     = {Representation learning using step-based deep multi-modal autoencoders},
  year      = {2019},
  issn      = {0031-3203},
  month     = nov,
  pages     = {12--23},
  volume    = {95},
  abstract  = {Deep learning techniques have been successfully used in learning a common representation for multi-view data, wherein different modalities are projected onto a common subspace. In a broader perspective, the techniques used to investigate common representation learning falls under the categories of ‘canonical correlation-based’ approaches and ‘autoencoder-based’ approaches. In this paper, we investigate the performance of deep autoencoder-based methods on multi-view data. We propose a novel step-based correlation multi-modal deep convolution neural network (CorrMCNN) which reconstructs one view of the data given the other while increasing the interaction between the representations at each hidden layer or every intermediate step. The idea of step reconstruction reduces the constraint of reconstruction of original data, instead, the objective function is optimized for reconstruction of representative features. This helps the proposed model to generalize for representation and transfer learning tasks efficiently for high dimensional data. Finally, we evaluate the performance of the proposed model on three multi-view and cross-modal problems viz., audio articulation, cross-modal image retrieval and multilingual (cross-language) document classification. Through extensive experiments, we find that the proposed model performs much better than the current state-of-the-art deep learning techniques on all three multi-view and cross-modal tasks.},
  doi       = {10.1016/j.patcog.2019.05.032},
  keywords  = {Convolution autoencoders,Multilingual document classification,Representation learning,Transfer learning},
  publisher = {Elsevier BV},
}

@Article{zhu2020,
  author    = {Wenhao Zhu and Xiaping Xu and Ke Yan and Shuang Liu and Xiaoya Yin},
  journal   = {IEEE Access},
  title     = {A Synchronized Word Representation Method With Dual Perceptual Information},
  year      = {2020},
  issn      = {2169-3536},
  pages     = {22335--22344},
  volume    = {8},
  abstract  = {The information used for human natural language comprehension is usually perceptual information, such as text, sounds, and images. In recent years, language models that learn semantics from single perceptual information sources (text) have gradually developed into multimodal language models that learn semantics from multiple perceptual information sources. Sound is perceptual information other than text that has been proven effective by many related works. However, there is still a need for further research on the incorporation method for perceptual information. Thus, this paper proposes a language model that synchronously trains dual perceptual information to enhance word representation. The representation is trained in a synchronized way that adopts an attention model to utilize both text and phonetic perceptual information in unsupervised learning tasks. On basis of that, these dual perceptual information is processed simultaneously, and that is similar with the cognitive process of human language understanding. The experiment results show that our approach achieve superior results in text classification and word similarity tasks with four languages of data set.},
  doi       = {10.1109/access.2020.2969983},
  keywords  = {Information representation,multi-layer neural network,natural language processing,unsupervised learning},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{carmona2020,
  author    = {Álvarez Carmona, Miguel Á. and Villatoro Tello, Esaú and Montes y Gómez, Manuel and Villaseñor Pineda, Luis},
  journal   = {Computación y Sistemas},
  title     = {Author Profiling in Social Media with Multimodal Information},
  year      = {2020},
  issn      = {1405-5546},
  month     = sep,
  number    = {3},
  pages     = {1289-1304},
  volume    = {24},
  abstract  = {This paper summarizes the thesis: "Author Profiling in Social Media with Multimodal Information." Our solution uses a multimodal approach to extracting information from written messages and images shared by users. Previous work has shown the existence of useful information for this task in these modalities; however, our proposal goes further, demonstrating the complementarity of the modalities when merging these two sources of information. To do this, we propose to transform images to texts, and with them, to have the same framework of representation for both kinds of information, which allow to achieve their fusion. Our work explores different methods for extracting information either from the text and the images. To represent the extracted information, different distributional term representations approaches were explored in order to identify the topics addressed by the user. For this purpose, an evaluation framework was proposed in order to identify the most appropriate method for this task. The results show that the textual descriptions of the images contain useful information for the author profiling task, and that the fusion of textual information with information extracted from the images increases the accuracy of this task.},
  doi       = {10.13053/cys-24-3-3488},
  issue     = {3},
  keywords  = {Author profiling,Multimodal information,Natural language processing,Text classification},
  publisher = {Instituto Politecnico Nacional/Centro de Investigacion en Computacion},
}

@InBook{doinychko2020,
  author    = {Doinychko, Anastasiia and Amini, Massih-Reza},
  pages     = {807--820},
  publisher = {Springer International Publishing},
  title     = {Biconditional Generative Adversarial Networks for Multiview Learning with Missing Views},
  year      = {2020},
  isbn      = {9783030454395},
  volume    = {12035 LNCS},
  abstract  = {In this paper, we present a conditional GAN with two generators and a common discriminator for multiview learning problems where observations have two views, but one of them may be missing for some of the training samples. This is for example the case for multilingual collections where documents are not available in all languages. Some studies tackled this problem by assuming the existence of view generation functions to approximately complete the missing views; for example Machine Translation to translate documents into the missing languages. These functions generally require an external resource to be set and their quality has a direct impact on the performance of the learned multiview classifier over the completed training set. Our proposed approach addresses this problem by jointly learning the missing views and the multiview classifier using a tripartite game with two generators and a discriminator. Each of the generators is associated to one of the views and tries to fool the discriminator by generating the other missing view conditionally on the corresponding observed view. The discriminator then tries to identify if for an observation, one of its views is completed by one of the generators or if both views are completed along with its class. Our results on a subset of Reuters RCV1/RCV2 collections show that the discriminator achieves significant classification performance; and that the generators learn the missing views with high quality without the need of any consequent external resource.},
  booktitle = {Advances in Information Retrieval},
  doi       = {10.1007/978-3-030-45439-5_53},
  issn      = {1611-3349},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords  = {Bilingual text classification,Conditional GAN,Multiview learning},
}

@InProceedings{hessel2020,
  author    = {Jack Hessel and Lillian Lee},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  title     = {Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!},
  year      = {2020},
  pages     = {861-877},
  publisher = {Association for Computational Linguistics (ACL)},
  abstract  = {Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task. This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure. For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.},
  doi       = {10.18653/v1/2020.emnlp-main.62},
  isbn      = {9781952148606},
  journal   = {EMNLP 2020 - 2020 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
}

@InBook{braz2020,
  author    = {Leodécio Braz and Vinicius Teixeira and Helio Pedrini and Zanoni Dias},
  pages     = {150--161},
  publisher = {Springer International Publishing},
  title     = {ImTeNet: Image-Text Classification Network for Abnormality Detection and Automatic Reporting on Musculoskeletal Radiographs},
  year      = {2020},
  isbn      = {9783030657758},
  volume    = {12558 LNBI},
  abstract  = {Deep learning techniques have been increasingly applied to provide more accurate results in the classification of medical images and in the classification and generation of report texts. The main objective of this paper is to investigate the influence of fusing several features of heterogeneous modalities to improve musculoskeletal abnormality detection in comparison with the individual results of image and text classification. In this work, we propose a novel image-text classification framework, named ImTeNet, to learn relevant features from image and text information for binary classification of musculoskeletal radiography. Initially, we use a caption generator model to artificially create textual data for a dataset lacking text information. Then, we apply the ImTeNet, a multi-modal information model that consists of two distinct networks, DenseNet-169 and BERT, to perform image and text classification tasks respectively, and a fusion module that receives a concatenation of feature vectors extracted from both. To evaluate our proposed approach, we used the Musculoskeletal Radiographs (MURA) dataset and compare the results obtained with image and text classification scheme individually.},
  booktitle = {Advances in Bioinformatics and Computational Biology},
  doi       = {10.1007/978-3-030-65775-8_14},
  issn      = {1611-3349},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
}

@Article{dacosta2022,
  author    = {Thiago Aparecido Gonçalves da Costa and Rodolfo Ipolito Meneguette and Jó Ueyama},
  journal   = {Expert Systems with Applications},
  title     = {Providing a greater precision of Situational Awareness of urban floods through Multimodal Fusion},
  year      = {2022},
  issn      = {0957-4174},
  month     = feb,
  pages     = {115923},
  volume    = {188},
  doi       = {10.1016/j.eswa.2021.115923},
  publisher = {Elsevier BV},
  url       = {https://linkinghub.elsevier.com/retrieve/pii/S095741742101277X},
}

@InProceedings{garg2021,
  author     = {Garg, Sugam and SS, Harichandana and Kumar, Sumit},
  booktitle  = {Proceedings of the 3rd ACM India Joint International Conference on Data Science &amp; Management of Data (8th ACM IKDD CODS &amp; 26th COMAD)},
  title      = {On-Device Document Classification using multimodal features},
  year       = {2021},
  month      = jan,
  pages      = {203--207},
  publisher  = {ACM},
  series     = {CODS COMAD 2021},
  city       = {New York, NY, USA},
  collection = {CODS COMAD 2021},
  doi        = {10.1145/3430984.3431030},
  isbn       = {9781450388177},
  journal    = {Proceedings of the 3rd ACM India Joint International Conference on Data Science & Management of Data (8th ACM IKDD CODS & 26th COMAD)},
}

@InProceedings{huc2021,
  author    = {Chenlong Hu and Yukun Feng and Hidetaka Kamigaito and Hiroya Takamura and Manabu Okumura},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  title     = {One-class Text Classification with Multi-modal Deep Support Vector Data Description},
  year      = {2021},
  pages     = {3378--3390},
  publisher = {Association for Computational Linguistics},
  city      = {Stroudsburg, PA, USA},
  doi       = {10.18653/v1/2021.eacl-main.296},
  journal   = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
}

@Article{gui2021,
  author    = {Meizeng Gui and Xueguo Xu},
  journal   = {IEEE Access},
  title     = {Technology Forecasting Using Deep Learning Neural Network: Taking the Case of Robotics},
  year      = {2021},
  issn      = {2169-3536},
  pages     = {53306--53316},
  volume    = {9},
  doi       = {10.1109/access.2021.3070105},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@InBook{gallo2021,
  author    = {Ignazio Gallo and Shah Nawaz and Nicola Landro and Riccardo La Grassainst},
  pages     = {339--352},
  publisher = {Springer International Publishing},
  title     = {Visual Word Embedding for Text Classification},
  year      = {2021},
  isbn      = {9783030687809},
  booktitle = {Pattern Recognition. ICPR International Workshops and Challenges},
  doi       = {10.1007/978-3-030-68780-9_29},
  issn      = {1611-3349},
}

@Article{setiawan2021,
  author    = {Setiawan, Esther},
  journal   = {International Journal of Intelligent Engineering and Systems},
  title     = {Multiview Sentiment Analysis with Image-Text-Concept Features of Indonesian Social Media Posts},
  year      = {2021},
  issn      = {2185-3118},
  month     = apr,
  number    = {2},
  pages     = {521--535},
  volume    = {14},
  abstract  = {Social media development makes it possible for everyone to express their opinions and information through text, speech, video, or images. Multiview sentiment analysis in current studies generally combines two modalities, text and image. It seeks to classify social media posts into two or more polarities, such as positive, neutral, or negative. To improve the performance of multiview Sentiment Analysis, we added another modality, which is concepts derived from text and image. Our proposed model integrates three views into a fusion with an ensemble approach by a metaclassifier. We performed text classification with Deep Convolutional Neural Networks. The input feature is Word2Vec for text representation in order to preserve semantic meaning. Additionally, we analyzed concepts from texts with SenticNet 5 as a knowledge base model and extracted concepts from images using the DeepSentiBank model. We obtained 2089 Adjective Noun Pairs and classified it with Multi-Layer Perceptron. Then we combined predicted probabilities from each classifier for Image, Text, and Concept by Ensemble Learning. A meta-classifier was implemented to predict the final sentiment from a fusion of Image-Text-Concept features. The fusion for multiview sentiment analysis works well and could achieve the best accuracy of 70% by applying the ensemble approach with Logistic Regression as the meta-classifier.},
  doi       = {10.22266/ijies2021.0430.47},
  issue     = {2},
  publisher = {The Intelligent Networks and Systems Society},
}

@Article{jiax2021,
  author    = {Jia, Xiaodong and Jing, Xiao-Yuan and Zhu, Xiaoke and Chen, Songcan and Du, Bo and Cai, Ziyun and He, Zhenyu and Yue, Dong},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title     = {Semi-Supervised Multi-View Deep Discriminant Representation Learning},
  year      = {2021},
  issn      = {1939-3539},
  month     = jul,
  number    = {7},
  pages     = {2496--2509},
  volume    = {43},
  abstract  = {Learning an expressive representation from multi-view data is a key step in various real-world applications. In this paper, we propose a semi-supervised multi-view deep discriminant representation learning (SMDDRL) approach. Unlike existing joint or alignment multi-view representation learning methods that cannot simultaneously utilize the consensus and complementary properties of multi-view data to learn inter-view shared and intra-view specific representations, SMDDRL comprehensively exploits the consensus and complementary properties as well as learns both shared and specific representations by employing the shared and specific representation learning network. Unlike existing shared and specific multi-view representation learning methods that ignore the redundancy problem in representation learning, SMDDRL incorporates the orthogonality and adversarial similarity constraints to reduce the redundancy of learned representations. Moreover, to exploit the information contained in unlabeled data, we design a semi-supervised learning framework by combining deep metric learning and density clustering. Experimental results on three typical multi-view learning tasks, i.e., webpage classification, image classification, and document classification demonstrate the effectiveness of the proposed approach.},
  doi       = {10.1109/tpami.2020.2973634},
  issue     = {7},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{sus2021,
  author    = {Shuzhi Su and Penglian Gao and Yanmin Zhu and Xingzhu Liang and Jun Xie},
  journal   = {IEEE Access},
  title     = {A Dynamic Discriminative Canonical Correlation Analysis via Adaptive Weight Scheme},
  year      = {2021},
  issn      = {2169-3536},
  pages     = {142653--142663},
  volume    = {9},
  abstract  = {In multi-view learning, massive literature is devoted to exploring the intrinsic structure between cross-views. It is well known that canonical correlation analysis (CCA) is a conventional multi-view learning method, which considers the correlation between two views. However, it fails to utilize class information and is difficult to suit different issues to extract discriminative features. In this paper, we propose a novel cross-view discriminative feature learning method called dynamic discriminative canonical correlation analysis, which captures class information to yield discriminative features. More specifically, we develop an adaptive weight scheme of cross-view within-class and between-class scatters to make full use of distribution class information. In addition, an iterative algorithm with Cauchy inequalities and the Lagrange multiplier is proposed to handle the non-smooth objective function. Our method is applied to face recognition and multi-linguistic text classification tasks. Extensive experimental results reveal that the adaptive weight scheme plays a beneficial role and our method is an effective feature learning.},
  doi       = {10.1109/access.2021.3118023},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{guelorget2021,
  author    = {Paul Guélorget and Guillaume Gadek and Titus Zaharia and Bruno Grilheres},
  journal   = {Procedia Computer Science},
  title     = {Active learning to measure opinion and violence in French newspapers},
  year      = {2021},
  issn      = {1877-0509},
  pages     = {202--211},
  volume    = {192},
  abstract  = {News articles analysis may be oversimplified when restricted to detecting classes of interest already benefiting from trustworthy labeled datasets, like political affiliation or fakeness. Behind an apparent neutrality, an editorial slant may be embodied by favoring one-sided interviews, avoiding topics or choosing oriented illustrations. These challenges, seen as machine learning problems, would require a tedious annotation task. We introduce ReALMS, an active learning framework capable of quickly elaborating models which detect arbitrary classes in multi-modal text and image documents. Evidence of this capability is given by a case study on French news outlets: the detection of subjectivity, demonstrations and violence.},
  doi       = {10.1016/j.procs.2021.08.021},
  publisher = {Elsevier BV},
}

@Article{liang2021,
  author    = {Yunji Liang and Huihui Li and Bin Guo and Zhiwen Yu and Xiaolong Zheng and Sagar Samtani and Daniel D. Zeng},
  journal   = {Information Sciences},
  title     = {Fusion of heterogeneous attention mechanisms in multi-view convolutional neural network for text classification},
  year      = {2021},
  issn      = {0020-0255},
  month     = feb,
  pages     = {295--312},
  volume    = {548},
  abstract  = {The rapid proliferation of user generated content has given rise to large volumes of text corpora. Increasingly, scholars, researchers, and organizations employ text classification to mine novel insights for high-impact applications. Despite their prevalence, conventional text classification methods rely on labor-intensive feature engineering efforts that are task specific, omit long-term relationships, and are not suitable for the rapidly evolving domains. While an increasing body of deep learning and attention mechanism literature aim to address these issues, extant methods often represent text as a single view and omit multiple sets of features at varying levels of granularity. Recognizing that these issues often result in performance degradations, we propose a novel Spatial View Attention Convolutional Neural Network (SVA-CNN). SVA-CNN leverages an innovative and carefully designed set of multi-view representation learning, a combination of heterogeneous attention mechanisms and CNN-based operations to automatically extract and weight multiple granularities and fine-grained representations. Rigorously evaluating SVA-CNN against prevailing text classification methods on five large-scale benchmark datasets indicates its ability to outperform extant deep learning-based classification methods in both performance and training time for document classification, sentiment analysis, and thematic identification applications. To facilitate model reproducibility and extensions, SVA-CNN's source code is also available via GitHub.},
  doi       = {10.1016/j.ins.2020.10.021},
  publisher = {Elsevier BV},
}

@Article{wang2021,
  author    = {Xinzhi Wang and Yudong Chang and Vijayan Sugumaran and Xiangfeng Luo and Peng Wang and Hui Zhang},
  journal   = {IEEE MultiMedia},
  title     = {Implicit Emotion Relationship Mining Based on Optimal and Majority Synthesis From Multimodal Data Prediction},
  year      = {2021},
  issn      = {1941-0166},
  month     = apr,
  number    = {2},
  pages     = {96--105},
  volume    = {28},
  abstract  = {Emotion is precious, useful for many applications such as public opinion detection and psychological disease prediction. Emotion recognition on multimodal data has attracted extensive attention. Although modifying model structure or multimodal feature fusion methods have contributed a lot to emotion recognition, little attention is paid to mining implicit emotion relationship. In this article, implicit emotion relationship consists of emotion distribution, confusion, and transfer. Emotion distribution allows multiple emotions in one sample, while confusion and transfer imply the prediction confusion and bias. In order to mine implicit emotion relationship in multimodal data, this article employs three image and two text classification models to recognize emotions, respectively. Two prediction emotion synthesis methods (optimal prediction emotion synthesis and majority prediction emotion synthesis) are proposed to synthesize the outputs of multiple models. Based on the results of two emotion synthesis methods, emotion distribution on samples is obtained. Emotion confusion and transfer among different emotion samples are analyzed by relative entropy and Jensen-Shannon divergence. Implicit emotion relationship mining has potential not only in the interpretation of model performance, but also in guiding the development of emotion recognition as prior knowledge. Finally, we take topic scenario as an instance to mine implicit emotion relationships.},
  doi       = {10.1109/mmul.2021.3071495},
  issue     = {2},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{zhang2021,
  author    = {You Zhang and Jin Wang and Xuejie Zhang},
  journal   = {Information Sciences},
  title     = {Learning sentiment sentence representation with multiview attention model},
  year      = {2021},
  issn      = {0020-0255},
  month     = sep,
  pages     = {459--474},
  volume    = {571},
  abstract  = {Self-attention mechanisms in deep neural networks, such as CNN, GRU and LSTM, have been proven to be effective for sentiment analysis. However, existing attention models tend to focus on individual tokens or aspect meanings in an expression. If a text contains information on multiple sentiments from different perspectives, the existing models will fail to extract the most critical and comprehensive features of the whole text. In the present study, a multiview attention model was proposed for learning sentence representation. Instead of using a single attention, multiple view vectors were used to map the attentions from different perspectives. Then, a fusion gate was adopted to combine these multiview attentions to draw a conclusion. To ensure the differences between multiview attentions, a regularization item was introduced to add a penalty to the loss function. In addition, the proposed model can be extended to other text tasks, such as questions and topics, to provide a comprehensive representation for the classification. Comparative experiments were conducted on both multiclass and multilabel classification datasets. The results revealed that the proposed method improves the performance of several previously proposed attention models.},
  doi       = {10.1016/j.ins.2021.05.044},
  publisher = {Elsevier BV},
}

@InProceedings{zingaro2021,
  author    = {Stefano Pio Zingaro and Giuseppe Lisanti and Maurizio Gabbrielli},
  booktitle = {2020 25th International Conference on Pattern Recognition (ICPR)},
  title     = {Multimodal Side- Tuning for Document Classification},
  year      = {2021},
  month     = jan,
  pages     = {5206--5213},
  publisher = {IEEE},
  abstract  = {In this paper, we propose to exploit the side-tuning framework for multimodal document classification. Side-tuning is a methodology for network adaptation recently introduced to solve some of the problems related to previous approaches. Thanks to this technique it is actually possible to overcome model rigidity and catastrophic forgetting of transfer learning by fine-tuning. The proposed solution uses off-the-shelf deep learning architectures leveraging the side-tuning framework to combine a base model with a tandem of two side networks. We show that side-tuning can be successfully employed also when different data sources are considered, e.g. text and images in document classification. The experimental results show that this approach pushes further the limit for document classification accuracy with respect to the state of the art.},
  doi       = {10.1109/icpr48806.2021.9413208},
  issn      = {1051-4651},
  journal   = {Proceedings - International Conference on Pattern Recognition},
}

@InProceedings{max2020,
  author    = {Xiao Ma and Peter Karkus and David Hsu and Wee Sun Lee},
  title     = {Particle Filter Recurrent Neural Networks},
  year      = {2020},
  month     = apr,
  number    = {04},
  pages     = {5101--5108},
  publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
  volume    = {34},
  abstract  = {Recurrent neural networks (RNNs) have been extraordinarily successful for prediction with sequential data. To tackle highly variable and multi-modal real-world data, we introduce Particle Filter Recurrent Neural Networks (PF-RNNs), a new RNN family that explicitly models uncertainty in its internal structure: while an RNN relies on a long, deterministic latent state vector, a PF-RNN maintains a latent state distribution, approximated as a set of particles. For effective learning, we provide a fully differentiable particle filter algorithm that updates the PF-RNN latent state distribution according to the Bayes rule. Experiments demonstrate that the proposed PF-RNNs outperform the corresponding standard gated RNNs on a synthetic robot localization dataset and 10 real-world sequence prediction datasets for text classification, stock price prediction, etc.},
  doi       = {10.1609/aaai.v34i04.5952},
  issn      = {2159-5399},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
}

@Article{maf2020,
  author   = {Fan Ma and Deyu Meng and Xuanyi Dong and Yi Yang},
  journal  = {Journal of Machine Learning Research},
  title    = {Self-paced multi-view co-training},
  year     = {2020},
  issn     = {1533-7928},
  volume   = {21},
  abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two or more different views and exchanges pseudo labels of unlabeled instances in an iterative way. During the co-training process, pseudo labels of unlabeled instances are very likely to be false especially in the initial training, while the standard co-training algorithm adopts a “draw without replacement” strategy and does not remove these wrongly labeled instances from training stages. Besides, most of the traditional co-training approaches are implemented for two-view cases, and their extensions in multi-view scenarios are not intuitive. These issues not only degenerate their performance as well as available application range but also hamper their fundamental theory. Moreover, there is no optimization model to explain the objective a co-training process manages to optimize. To address these issues, in this study we design a unified self-paced multi-view co-training (SPamCo) framework which draws unlabeled instances with replacement. Two specified co-regularization terms are formulated to develop different strategies for selecting pseudo-labeled instances during training. Both forms share the same optimization strategy which is consistent with the iteration process in co-training and can be naturally extended to multi-view scenarios. A distributed optimization strategy is also introduced to train the classifier of each view in parallel to further improve the efficiency of the algorithm. Furthermore, the SPamCo algorithm is proved to be PAC learnable, supporting its theoretical soundness. Experiments conducted on synthetic, text categorization, person re-identification, image recognition and object detection data sets substantiate the superiority of the proposed method.},
}

@InProceedings{lij2020,
  author   = {Jinfeng Li and Tianyu Du and S. Ji and Rong Zhang and Quan Lu and Min Yang and Ting Wang},
  title    = {TextShield: Robust Text Classification Based on Multimodal Embedding and Neural Machine Translation},
  year     = {2020},
  abstract = {Text-based toxic content detection is an important tool for reducing harmful interactions in online social media environments. Yet, its underlying mechanism, deep learning-based text classification (DLTC), is inherently vulnerable to maliciously crafted adversarial texts. To mitigate such vulnerabilities, intensive research has been conducted on strengthening English-based DLTC models. However, the existing defenses are not effective for Chinese-based DLTC models, due to the unique sparseness, diversity, and variation of the Chinese language. In this paper, we bridge this striking gap by presenting T EXT S HIELD , a new adversarial defense framework specifically designed for Chinese-based DLTC models. T EXT S HIELD differs from previous work in several key aspects: (i) generic – it applies to any Chinese-based DLTC models without requiring re-training; (ii) robust – it signifi-cantly reduces the attack success rate even under the setting of adaptive attacks; and (iii) accurate – it has little impact on the performance of DLTC models over legitimate inputs. Extensive evaluations show that it outperforms both existing methods and the industry-leading platforms. Future work will explore its applicability in broader practical tasks.},
  journal  = {Proceedings of the 29th USENIX Security Symposium},
  url      = {https://www.semanticscholar.org/paper/93ad61c5700b3aabbc9192f742fea1a734bcb45b},
  venue    = {USENIX Security Symposium},
}

@InProceedings{ma2021,
  author    = {Chunpeng Ma and Aili Shen and Hiyori Yoshikawa and Tomoya Iwakura and Daniel Beck and Timothy Baldwin},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  title     = {On the (In)Effectiveness of Images for Text Classification},
  year      = {2021},
  pages     = {42--48},
  publisher = {Association for Computational Linguistics},
  abstract  = {Images are core components of multi-modal learning in natural language processing (NLP), and results have varied substantially as to whether images improve NLP tasks or not. One confounding effect has been that previous NLP research has generally focused on sophisticated tasks (in varying settings), generally applied to English only. We focus on text classification, in the context of assigning named entity classes to a given Wikipedia page, where images generally complement the text and the Wikipedia page can be in one of a number of different languages. Our experiments across a range of languages show that images complement NLP models (including BERT) trained without external pre-training, but when combined with BERT models pre-trained on large-scale external data, images contribute nothing.},
  doi       = {10.18653/v1/2021.eacl-main.4},
  journal   = {EACL 2021 - 16th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference},
}
